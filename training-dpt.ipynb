{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11679104,"sourceType":"datasetVersion","datasetId":7330224},{"sourceId":11694607,"sourceType":"datasetVersion","datasetId":7340064}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/isl-org/DPT.git\n\n# Download models and weights\n#!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt\n#!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\n#!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-ade20k-53898607.pt\n!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-ade20k-b12dca68.pt\n    \n# Import weights\n#!mv ./dpt_hybrid-ade20k-53898607.pt ./DPT/weights\n!mv ./dpt_large-ade20k-b12dca68.pt ./DPT/weights\n#!mv ./dpt_large-midas-2f21e586.pt ./DPT/weights\n#!mv ./dpt_hybrid-midas-501f0c75.pt ./DPT/weights\n\n# Pip install required libraries with last releases\n!pip install torch\n!pip install torchvision\n!pip install opencv-python\n!pip install timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:46:45.067591Z","iopub.execute_input":"2025-05-06T04:46:45.067802Z","iopub.status.idle":"2025-05-06T04:48:38.169783Z","shell.execute_reply.started":"2025-05-06T04:46:45.067783Z","shell.execute_reply":"2025-05-06T04:48:38.168829Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:48:55.471828Z","iopub.execute_input":"2025-05-06T04:48:55.472260Z","iopub.status.idle":"2025-05-06T04:48:55.595637Z","shell.execute_reply.started":"2025-05-06T04:48:55.472230Z","shell.execute_reply":"2025-05-06T04:48:55.594739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd ../input/dataset-v1/d1/ && ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:48:58.531621Z","iopub.execute_input":"2025-05-06T04:48:58.531974Z","iopub.status.idle":"2025-05-06T04:48:58.666879Z","shell.execute_reply.started":"2025-05-06T04:48:58.531949Z","shell.execute_reply":"2025-05-06T04:48:58.665951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os \ndef initialize_system():\n    # Generate output directory\n    if(not(os.path.isdir('/kaggle/working/output'))):\n        os.mkdir('/kaggle/working/output')\n    \n    filename = \"/kaggle/working/DPT/run_monodepth.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"output_monodepth\"', '\"/kaggle/working/DPT/output_monodepth\"'))\n\n    filename = \"/kaggle/working/DPT/run_segmentation.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"output_semseg\"', '\"/kaggle/working/DPT/output_semseg\"'))\n    \n    filename = \"/kaggle/working/DPT/run_monodepth.py\"\n    text = open(filename).read()\n\n    #Here goes your files\n    open(filename, \"w+\").write(text.replace('\"input\"', '\"/kaggle/working/DPT/input/\"'))\n\n    filename = \"/kaggle/working/DPT/run_segmentation.py\"\n    text = open(filename).read()\n\n    #Here goes your files\n    open(filename, \"w+\").write(text.replace('\"input\"', '\"/kaggle/working/DPT/input/\"'))\n    \n    filename = \"/kaggle/working/DPT/run_monodepth.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"weights/', '\"/kaggle/working/DPT/weights/'))\n\n    filename = \"/kaggle/working/DPT/run_segmentation.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"weights/', '\"/kaggle/working/DPT/weights/'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:49:03.922837Z","iopub.execute_input":"2025-05-06T04:49:03.923146Z","iopub.status.idle":"2025-05-06T04:49:03.931422Z","shell.execute_reply.started":"2025-05-06T04:49:03.923120Z","shell.execute_reply":"2025-05-06T04:49:03.930710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"initialize_system()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:49:06.769986Z","iopub.execute_input":"2025-05-06T04:49:06.770267Z","iopub.status.idle":"2025-05-06T04:49:06.777201Z","shell.execute_reply.started":"2025-05-06T04:49:06.770246Z","shell.execute_reply":"2025-05-06T04:49:06.776340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"DPT/dpt/base_model.py\", \"r\") as f:\n    code = f.read()\n\ncode = code.replace(\n    \"self.load_state_dict(parameters)\",\n    \"\"\"own_state = self.state_dict()\n        filtered = {k: v for k, v in parameters.items() if k in own_state and v.shape == own_state[k].shape}\n        print(f\"Cargando {len(filtered)} de {len(own_state)} parámetros del checkpoint.\")\n        self.load_state_dict(filtered, strict=False)\"\"\"\n)\n\n\nwith open(\"DPT/dpt/base_model.py\", \"w\") as f:\n    f.write(code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:49:17.823544Z","iopub.execute_input":"2025-05-06T04:49:17.824159Z","iopub.status.idle":"2025-05-06T04:49:17.829411Z","shell.execute_reply.started":"2025-05-06T04:49:17.824133Z","shell.execute_reply":"2025-05-06T04:49:17.828726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Leer contenido original\nwith open(\"DPT/dpt/models.py\", \"r\") as f:\n    code = f.read()\n\n# Añadir el nuevo método seguro al final del archivo si no existe\nif \"def load_partial_weights\" not in code:\n    code += \"\"\"\n\n    def load_partial_weights(self, path):\n        parameters = torch.load(path, map_location=\"cpu\")\n        if \"model\" in parameters:\n            parameters = parameters[\"model\"]\n\n        own_state = self.state_dict()\n        filtered = {\n            k: v for k, v in parameters.items()\n            if k in own_state and v.shape == own_state[k].shape\n        }\n\n        print(f\"Cargando {len(filtered)} de {len(own_state)} parámetros del checkpoint.\")\n        self.load_state_dict(filtered, strict=False)\n\"\"\"\n\n\n\n# Guardar cambios\nwith open(\"DPT/dpt/models.py\", \"w\") as f:\n    f.write(code)\n\nprint(\"Archivo 'models.py' modificado exitosamente.\")\n\n\n!cat DPT/dpt/models.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:49:20.037231Z","iopub.execute_input":"2025-05-06T04:49:20.037979Z","iopub.status.idle":"2025-05-06T04:49:20.163280Z","shell.execute_reply.started":"2025-05-06T04:49:20.037953Z","shell.execute_reply":"2025-05-06T04:49:20.162460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cat DPT/dpt/base_model.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:49:25.789838Z","iopub.execute_input":"2025-05-06T04:49:25.790262Z","iopub.status.idle":"2025-05-06T04:49:25.911866Z","shell.execute_reply.started":"2025-05-06T04:49:25.790232Z","shell.execute_reply":"2025-05-06T04:49:25.910844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# IMPORTS\n# ===============================\nimport os\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader,random_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.metrics import f1_score\nfrom DPT.dpt.models import DPTSegmentationModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:49:30.458970Z","iopub.execute_input":"2025-05-06T04:49:30.459300Z","iopub.status.idle":"2025-05-06T04:49:42.472725Z","shell.execute_reply.started":"2025-05-06T04:49:30.459272Z","shell.execute_reply":"2025-05-06T04:49:42.471875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# DATASET CON ALBUMENTATIONS\n# ===============================\nclass RoadSignSegmentationDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.samples = []\n        self.transform = transform\n\n        subdirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir)\n                   if os.path.isdir(os.path.join(root_dir, d))]\n\n        for subdir in subdirs:\n            for fname in os.listdir(subdir):\n                if fname.endswith(\"_img.png\"):\n                    base = fname.replace(\"_img.png\", \"\")\n                    img_path = os.path.join(subdir, f\"{base}_img.png\")\n                    mask_path = os.path.join(subdir, f\"{base}_label.png\")\n                    label_names_path = os.path.join(subdir, f\"{base}_label_names.txt\")\n\n                    if os.path.exists(mask_path):\n                        label_names = []\n                        if os.path.exists(label_names_path):\n                            with open(label_names_path, 'r') as f:\n                                label_names = [line.strip() for line in f.readlines()]\n\n                        self.samples.append({\n                            \"image\": img_path,\n                            \"mask\": mask_path,\n                            \"labels\": label_names\n                        })\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        image = np.array(Image.open(sample[\"image\"]).convert(\"RGB\"))\n        mask = np.array(Image.open(sample[\"mask\"]))\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented[\"image\"]\n            mask = augmented[\"mask\"].long()\n\n        return image, mask, sample[\"labels\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:49:57.323617Z","iopub.execute_input":"2025-05-06T04:49:57.324011Z","iopub.status.idle":"2025-05-06T04:49:57.337088Z","shell.execute_reply.started":"2025-05-06T04:49:57.323984Z","shell.execute_reply":"2025-05-06T04:49:57.336154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# TRANSFORMACIONES\n# ===============================\ntrain_transform = A.Compose([\n    A.Resize(512, 512),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.GaussNoise(p=0.2),\n    A.Affine(\n        scale=(0.8, 1.2),\n        translate_percent={\"x\": 0.05, \"y\": 0.05},\n        rotate=(-15, 15),\n        p=0.5\n    ),\n    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ToTensorV2()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:50:01.217526Z","iopub.execute_input":"2025-05-06T04:50:01.218320Z","iopub.status.idle":"2025-05-06T04:50:01.228350Z","shell.execute_reply.started":"2025-05-06T04:50:01.218293Z","shell.execute_reply":"2025-05-06T04:50:01.227467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# MÉTRICAS\n# ===============================\ndef compute_iou(pred, target, num_classes=2):\n    ious = []\n    for cls in range(num_classes):\n        pred_inds = pred == cls\n        target_inds = target == cls\n        intersection = (pred_inds & target_inds).sum()\n        union = (pred_inds | target_inds).sum()\n        if union == 0:\n            ious.append(np.nan)\n        else:\n            ious.append(intersection / union)\n    return np.nanmean(ious)\n\ndef pixel_accuracy(pred, target):\n    correct = (pred == target).sum()\n    total = target.size\n    return correct / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:50:04.259028Z","iopub.execute_input":"2025-05-06T04:50:04.259305Z","iopub.status.idle":"2025-05-06T04:50:04.265103Z","shell.execute_reply.started":"2025-05-06T04:50:04.259286Z","shell.execute_reply":"2025-05-06T04:50:04.264227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# DATASET\n# ===============================\ndataset = RoadSignSegmentationDataset(\n    root_dir=\"../input/dataset-v2/d2/\",\n    transform=train_transform\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:51:01.072286Z","iopub.execute_input":"2025-05-06T04:51:01.073130Z","iopub.status.idle":"2025-05-06T04:51:02.217585Z","shell.execute_reply.started":"2025-05-06T04:51:01.073105Z","shell.execute_reply":"2025-05-06T04:51:02.216719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for image, mask, labels in dataset:\n    print(labels)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:51:03.748607Z","iopub.execute_input":"2025-05-06T04:51:03.749587Z","iopub.status.idle":"2025-05-06T04:51:04.554611Z","shell.execute_reply.started":"2025-05-06T04:51:03.749557Z","shell.execute_reply":"2025-05-06T04:51:04.553866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# CONFIGURACIÓN\n# ===============================\nNUM_CLASSES = 4\nBATCH_SIZE = 4\nNUM_EPOCHS = 15\nLEARNING_RATE = 1e-4\nVAL_SPLIT = 0.2\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ===============================\n# DATALOADER\n# ===============================\nval_size = int(VAL_SPLIT * len(dataset))\ntrain_size = len(dataset) - val_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(len(train_dataset))\nprint(len(val_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:51:41.675984Z","iopub.execute_input":"2025-05-06T04:51:41.676389Z","iopub.status.idle":"2025-05-06T04:51:41.683337Z","shell.execute_reply.started":"2025-05-06T04:51:41.676361Z","shell.execute_reply":"2025-05-06T04:51:41.682420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# MODELO\n# ===============================\nmodel = DPTSegmentationModel(\n    num_classes=NUM_CLASSES,\n    backbone=\"vitl16_384\",\n    readout=\"project\",\n    features=256,\n    use_bn=True\n)\n\nmodel.load_partial_weights(\"DPT/weights/dpt_large-ade20k-b12dca68.pt\")\nmodel.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:51:46.093077Z","iopub.execute_input":"2025-05-06T04:51:46.093389Z","iopub.status.idle":"2025-05-06T04:51:53.030852Z","shell.execute_reply.started":"2025-05-06T04:51:46.093369Z","shell.execute_reply":"2025-05-06T04:51:53.029956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# CONGELAR BACKBONE\n# ===============================\nfor param in model.pretrained.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:52:04.910447Z","iopub.execute_input":"2025-05-06T04:52:04.910809Z","iopub.status.idle":"2025-05-06T04:52:04.916367Z","shell.execute_reply.started":"2025-05-06T04:52:04.910787Z","shell.execute_reply":"2025-05-06T04:52:04.915418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:52:08.807984Z","iopub.execute_input":"2025-05-06T04:52:08.808643Z","iopub.status.idle":"2025-05-06T04:52:08.813091Z","shell.execute_reply.started":"2025-05-06T04:52:08.808617Z","shell.execute_reply":"2025-05-06T04:52:08.812072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# ENTRENAMIENTO (CONGELADO)\n# ===============================\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\nmetrics = {\n    \"train_loss\": [], \"train_iou\": [], \"train_acc\": [], \"train_f1\": [],\n    \"val_loss\": [], \"val_iou\": [], \"val_acc\": [], \"val_f1\": []\n}\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n    train_ious, train_accs, train_f1s = [], [], []\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}\"):\n        images, masks = images.to(DEVICE), masks.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n        preds = torch.argmax(outputs, dim=1)\n        for p, t in zip(preds, masks):\n            p_np, t_np = p.cpu().numpy(), t.cpu().numpy()\n            train_ious.append(compute_iou(p_np, t_np))\n            train_accs.append(pixel_accuracy(p_np, t_np))\n            train_f1s.append(f1_score(t_np.flatten(), p_np.flatten(), average=\"macro\"))\n\n    metrics[\"train_loss\"].append(running_loss / len(train_loader))\n    metrics[\"train_iou\"].append(np.nanmean(train_ious))\n    metrics[\"train_acc\"].append(np.mean(train_accs))\n    metrics[\"train_f1\"].append(np.mean(train_f1s))\n\n    # ===============================\n    # VALIDACIÓN\n    # ===============================\n    model.eval()\n    val_loss = 0.0\n    ious, accs, f1s = [], [], []\n\n    with torch.no_grad():\n        for images, masks, _ in val_loader:\n            images, masks = images.to(DEVICE), masks.to(DEVICE)\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            val_loss += loss.item()\n\n            preds = torch.argmax(outputs, dim=1)\n            for p, t in zip(preds, masks):\n                p_np, t_np = p.cpu().numpy(), t.cpu().numpy()\n                ious.append(compute_iou(p_np, t_np))\n                accs.append(pixel_accuracy(p_np, t_np))\n                f1s.append(f1_score(t_np.flatten(), p_np.flatten(), average=\"macro\"))\n\n    metrics[\"val_loss\"].append(val_loss / len(val_loader))\n    metrics[\"val_iou\"].append(np.nanmean(ious))\n    metrics[\"val_acc\"].append(np.mean(accs))\n    metrics[\"val_f1\"].append(np.mean(f1s))\n\n    print(f\"[Train] Epoch {epoch+1} | Loss: {metrics['train_loss'][-1]:.4f} | IoU: {metrics['train_iou'][-1]:.4f} | Acc: {metrics['train_acc'][-1]:.4f} | F1: {metrics['train_f1'][-1]:.4f}\")\n    print(f\"[Val]   Epoch {epoch+1} | Loss: {metrics['val_loss'][-1]:.4f} | IoU: {metrics['val_iou'][-1]:.4f} | Acc: {metrics['val_acc'][-1]:.4f} | F1: {metrics['val_f1'][-1]:.4f}\")\n\n    scheduler.step()\n\n# ===============================\n# GUARDAR MODELO\n# ===============================\nos.makedirs(\"checkpoints\", exist_ok=True)\ntorch.save(model.state_dict(), \"checkpoints/dpt_finetuned.pt\")\nprint(\"Modelo guardado en checkpoints/dpt_finetuned.pt\")\n\n# ===============================\n# GRÁFICAS\n# ===============================\nplt.figure(figsize=(12, 8))\nplt.plot(metrics[\"train_loss\"], label=\"Train Loss\")\nplt.plot(metrics[\"val_loss\"], label=\"Val Loss\")\nplt.plot(metrics[\"train_iou\"], label=\"Train IoU\")\nplt.plot(metrics[\"val_iou\"], label=\"Val IoU\")\nplt.plot(metrics[\"train_acc\"], label=\"Train Accuracy\")\nplt.plot(metrics[\"val_acc\"], label=\"Val Accuracy\")\nplt.plot(metrics[\"train_f1\"], label=\"Train F1\")\nplt.plot(metrics[\"val_f1\"], label=\"Val F1\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Valor\")\nplt.title(\"Métricas de Entrenamiento y Validación\")\nplt.grid(True)\nplt.legend()\nplt.savefig(\"checkpoints/metrics_plot.png\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T04:52:11.706888Z","iopub.execute_input":"2025-05-06T04:52:11.707658Z","iopub.status.idle":"2025-05-06T05:46:22.500356Z","shell.execute_reply.started":"2025-05-06T04:52:11.707634Z","shell.execute_reply":"2025-05-06T05:46:22.499390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n\n# Asegura carpeta de salida\nos.makedirs(\"checkpoints/metric_plots\", exist_ok=True)\n\ndef plot_metric(train_values, val_values, title, ylabel, filename):\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_values, label=\"Train\")\n    plt.plot(val_values, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(f\"checkpoints/metric_plots/{filename}.png\")\n    plt.show()\n\n# 1. Pérdida (Loss) para detectar overfitting/underfitting\nplot_metric(\n    metrics[\"train_loss\"],\n    metrics[\"val_loss\"],\n    \"Evolución de la Pérdida - ¿Hay overfitting o underfitting?\",\n    \"CrossEntropy Loss\",\n    \"loss_comparison\"\n)\n\n# 2. IoU (Intersección sobre Unión) - Métrica clave para segmentación\nplot_metric(\n    metrics[\"train_iou\"],\n    metrics[\"val_iou\"],\n    \"Evolución del IoU - Precisión espacial de la segmentación\",\n    \"IoU\",\n    \"iou_comparison\"\n)\n\n# 3. Pixel Accuracy - Proporción de píxeles correctamente clasificados\nplot_metric(\n    metrics[\"train_acc\"],\n    metrics[\"val_acc\"],\n    \"Exactitud por píxel - ¿Qué tan bien clasifica cada píxel?\",\n    \"Pixel Accuracy\",\n    \"accuracy_comparison\"\n)\n\n# 4. F1 Score - Balance entre precisión y recall\nplot_metric(\n    metrics[\"train_f1\"],\n    metrics[\"val_f1\"],\n    \"Evolución del F1 Score - Equilibrio entre precisión y cobertura\",\n    \"F1 Score\",\n    \"f1_comparison\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T05:47:04.093209Z","iopub.execute_input":"2025-05-06T05:47:04.093524Z","iopub.status.idle":"2025-05-06T05:47:05.430303Z","shell.execute_reply.started":"2025-05-06T05:47:04.093502Z","shell.execute_reply":"2025-05-06T05:47:05.429578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(model, dataloader, device, num_classes=2, normalize=True, title=\"Matriz de Confusión\"):\n    all_preds = []\n    all_targets = []\n\n    model.eval()\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy().flatten())\n            all_targets.extend(masks.cpu().numpy().flatten())\n\n    cm = confusion_matrix(all_targets, all_preds, labels=list(range(num_classes)), normalize='true' if normalize else None)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f\"Clase {i}\" for i in range(num_classes)])\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.grid(False)\n    plt.savefig(\"checkpoints/confusion_matrix.png\")\n    plt.show()\n\nplot_confusion_matrix(model, val_loader, DEVICE, num_classes=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T05:47:13.493079Z","iopub.execute_input":"2025-05-06T05:47:13.493601Z","iopub.status.idle":"2025-05-06T05:47:57.597199Z","shell.execute_reply.started":"2025-05-06T05:47:13.493575Z","shell.execute_reply":"2025-05-06T05:47:57.596386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# EVALUADOR DE MODELO\n# ===============================\ndef evaluate_model(model_path, dataloader, device, num_classes=2, pretrained=True):\n    if pretrained:\n        model = DPTSegmentationModel(\n            num_classes=num_classes,\n            path=model_path,\n            backbone=\"vitl16_384\",\n        )\n    else:\n        model = DPTSegmentationModel(\n            num_classes=num_classes,\n            path=None,\n            backbone=\"vitl16_384\",\n        )\n        model.load_partial_weights(model_path)\n\n    model.to(device)\n    model.eval()\n\n    iou_scores = []\n    pixel_accuracies = []\n    f1_scores = []\n\n    with torch.no_grad():\n        for images, masks, labels in tqdm(dataloader, desc=f\"Evaluando {model_path}\"):\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n\n            for p, t in zip(preds, masks):\n                p_np = p.cpu().numpy()\n                t_np = t.cpu().numpy()\n                iou_scores.append(compute_iou(p_np, t_np, num_classes))\n                pixel_accuracies.append(pixel_accuracy(p_np, t_np))\n                f1_scores.append(f1_score(t_np.flatten(), p_np.flatten(), average=\"macro\"))\n\n    return {\n        \"IoU\": np.nanmean(iou_scores),\n        \"Pixel Accuracy\": np.mean(pixel_accuracies),\n        \"F1 Score\": np.mean(f1_scores)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T05:48:53.908645Z","iopub.execute_input":"2025-05-06T05:48:53.909064Z","iopub.status.idle":"2025-05-06T05:48:53.919381Z","shell.execute_reply.started":"2025-05-06T05:48:53.909034Z","shell.execute_reply":"2025-05-06T05:48:53.918504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_transform = A.Compose([\n    A.Resize(512, 512),\n    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ToTensorV2()\n])\n\nDATASET_PATH = \"../input/dataset-v1/d1/\"\n\neval_dataset = RoadSignSegmentationDataset(DATASET_PATH, transform=eval_transform)\neval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Evaluación\nprint(\"Evaluando modelo preentrenado...\")\nresults_pre = evaluate_model(\"DPT/weights/dpt_large-ade20k-b12dca68.pt\", eval_loader, DEVICE, NUM_CLASSES, pretrained=True)\nprint(\"Modelo Preentrenado:\", results_pre)\n\nprint(\"Evaluando modelo fine-tuned...\")\nresults_fine = evaluate_model(\"checkpoints/dpt_finetuned.pt\", eval_loader, DEVICE, NUM_CLASSES, pretrained=False)\nprint(\"Modelo Fine-Tuned:\", results_fine)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T05:48:56.417745Z","iopub.execute_input":"2025-05-06T05:48:56.418038Z","iopub.status.idle":"2025-05-06T05:50:16.530804Z","shell.execute_reply.started":"2025-05-06T05:48:56.418017Z","shell.execute_reply":"2025-05-06T05:50:16.529644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================\n# INTERACTIVO: ELECCIÓN DE MODELO + SUBIDA DE IMAGEN\n# ================================================\nfrom IPython.display import display\nfrom ipywidgets import Dropdown, FileUpload\nimport io\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as T\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# -------- Dropdown para elegir modelo ----------\nmodel_selector = Dropdown(\n    options=[('Fine-tuned (model)', 'model'), ('Preentrenado (model2)', 'model2')],\n    value='model',\n    description='Modelo:',\n)\ndisplay(model_selector)\n\n# -------- Subida de imagen ----------\nuploader = FileUpload(accept='image/*', multiple=False)\ndisplay(uploader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T05:50:55.642321Z","iopub.execute_input":"2025-05-06T05:50:55.642650Z","iopub.status.idle":"2025-05-06T05:50:55.656332Z","shell.execute_reply.started":"2025-05-06T05:50:55.642629Z","shell.execute_reply":"2025-05-06T05:50:55.655539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================\n# PROCESAMIENTO Y PREDICCIÓN - VERSIÓN ROBUSTA\n# ================================================\n\nif uploader.value:\n    # Detectar tipo de estructura\n    if isinstance(uploader.value, dict):\n        uploaded_file = list(uploader.value.values())[0]\n    elif isinstance(uploader.value, tuple):\n        uploaded_file = uploader.value[0]\n    else:\n        raise ValueError(\"Formato de archivo no reconocido\")\n\n    # Obtener imagen\n    image_bytes = uploaded_file['content']\n    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n    orig_image = image.copy()\n\n    # Transformación\n    transform = T.Compose([\n        T.Resize((512, 512)),\n        T.ToTensor(),\n        T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ])\n    input_tensor = transform(image).unsqueeze(0).to(DEVICE)\n\n    # Selección de modelo\n    selected_model = model if model_selector.value == 'model' else model2\n    selected_model.to(DEVICE)\n    selected_model.eval()\n\n    with torch.no_grad():\n        output = selected_model(input_tensor)\n        pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n\n    # Visualización\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.imshow(orig_image)\n    plt.title(\"Imagen Original\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(pred_mask, cmap=\"gray\")\n    plt.title(\"Máscara Predicha\")\n    plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nelse:\n    print(\"⬆️ Subí una imagen para visualizar la predicción.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T05:52:23.347939Z","iopub.execute_input":"2025-05-06T05:52:23.348524Z","iopub.status.idle":"2025-05-06T05:52:25.382419Z","shell.execute_reply.started":"2025-05-06T05:52:23.348496Z","shell.execute_reply":"2025-05-06T05:52:25.381502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Descongelar todo\nfor param in model.parameters():\n    param.requires_grad = True\n\n# O: solo descongelar encoder\nfor param in model.pretrained.parameters():\n    param.requires_grad = True\n\n# Nuevo optimizador con lr más bajo\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:41:02.188178Z","iopub.execute_input":"2025-05-05T05:41:02.188381Z","iopub.status.idle":"2025-05-05T05:41:02.196334Z","shell.execute_reply.started":"2025-05-05T05:41:02.188366Z","shell.execute_reply":"2025-05-05T05:41:02.195595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}