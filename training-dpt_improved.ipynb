{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11679104,"sourceType":"datasetVersion","datasetId":7330224},{"sourceId":11694607,"sourceType":"datasetVersion","datasetId":7340064}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"DELETE = True\nif DELETE:\n    !rm -rf *\n    print(\"Delete all...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:50:32.418551Z","iopub.execute_input":"2025-06-14T13:50:32.418727Z","iopub.status.idle":"2025-06-14T13:50:33.140442Z","shell.execute_reply.started":"2025-06-14T13:50:32.418709Z","shell.execute_reply":"2025-06-14T13:50:33.139580Z"}},"outputs":[{"name":"stdout","text":"Delete all...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install gputil\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:50:33.142799Z","iopub.execute_input":"2025-06-14T13:50:33.143445Z","iopub.status.idle":"2025-06-14T13:50:38.882816Z","shell.execute_reply.started":"2025-06-14T13:50:33.143409Z","shell.execute_reply":"2025-06-14T13:50:38.882166Z"}},"outputs":[{"name":"stdout","text":"Collecting gputil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: gputil\n  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=3afc05e36c284b1984bc0227b782f023dce8ff8d8b26c46ae46ae7c777c88a39\n  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\nSuccessfully built gputil\nInstalling collected packages: gputil\nSuccessfully installed gputil-1.4.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!git clone https://github.com/isl-org/DPT.git\n\n# Download models and weights\n#!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt\n#!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\n#!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-ade20k-53898607.pt\n!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-ade20k-b12dca68.pt\n    \n# Import weights\n#!mv ./dpt_hybrid-ade20k-53898607.pt ./DPT/weights\n!mv ./dpt_large-ade20k-b12dca68.pt ./DPT/weights\n#!mv ./dpt_large-midas-2f21e586.pt ./DPT/weights\n#!mv ./dpt_hybrid-midas-501f0c75.pt ./DPT/weights\n\n# Pip install required libraries with last releases\n!pip install torch\n!pip install torchvision\n!pip install opencv-python\n!pip install timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:50:38.883806Z","iopub.execute_input":"2025-06-14T13:50:38.884096Z","iopub.status.idle":"2025-06-14T13:52:25.303925Z","shell.execute_reply.started":"2025-06-14T13:50:38.884047Z","shell.execute_reply":"2025-06-14T13:52:25.303097Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Cloning into 'DPT'...\nremote: Enumerating objects: 782, done.\u001b[K\nremote: Counting objects: 100% (176/176), done.\u001b[K\nremote: Compressing objects: 100% (62/62), done.\u001b[K\nremote: Total 782 (delta 144), reused 114 (delta 114), pack-reused 606 (from 1)\u001b[K\nReceiving objects: 100% (782/782), 458.16 KiB | 5.59 MiB/s, done.\nResolving deltas: 100% (371/371), done.\n--2025-06-14 13:50:39--  https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-ade20k-b12dca68.pt\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/isl-org/DPT/releases/download/1_0/dpt_large-ade20k-b12dca68.pt [following]\n--2025-06-14 13:50:39--  https://github.com/isl-org/DPT/releases/download/1_0/dpt_large-ade20k-b12dca68.pt\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/350409920/161d7b80-8b44-11eb-8760-b3525312b581?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250614%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250614T135039Z&X-Amz-Expires=300&X-Amz-Signature=f1796d98510010e928e338a9d36ea899cad92ca777effbc819b5bba0df1f3bbe&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Ddpt_large-ade20k-b12dca68.pt&response-content-type=application%2Foctet-stream [following]\n--2025-06-14 13:50:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/350409920/161d7b80-8b44-11eb-8760-b3525312b581?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250614%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250614T135039Z&X-Amz-Expires=300&X-Amz-Signature=f1796d98510010e928e338a9d36ea899cad92ca777effbc819b5bba0df1f3bbe&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Ddpt_large-ade20k-b12dca68.pt&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1380151633 (1.3G) [application/octet-stream]\nSaving to: ‘dpt_large-ade20k-b12dca68.pt’\n\ndpt_large-ade20k-b1 100%[===================>]   1.29G   100MB/s    in 8.6s    \n\n2025-06-14 13:50:48 (153 MB/s) - ‘dpt_large-ade20k-b12dca68.pt’ saved [1380151633/1380151633]\n\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.305042Z","iopub.execute_input":"2025-06-14T13:52:25.305271Z","iopub.status.idle":"2025-06-14T13:52:25.423636Z","shell.execute_reply.started":"2025-06-14T13:52:25.305247Z","shell.execute_reply":"2025-06-14T13:52:25.423010Z"}},"outputs":[{"name":"stdout","text":"DPT\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!cd ../input/dataset-v1/d1/ && ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.424548Z","iopub.execute_input":"2025-06-14T13:52:25.424732Z","iopub.status.idle":"2025-06-14T13:52:25.550909Z","shell.execute_reply.started":"2025-06-14T13:52:25.424712Z","shell.execute_reply":"2025-06-14T13:52:25.550105Z"}},"outputs":[{"name":"stdout","text":"informative_signal  prevention_sign  regulation_sign\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os \ndef initialize_system():\n    # Generate output directory\n    if(not(os.path.isdir('/kaggle/working/output'))):\n        os.mkdir('/kaggle/working/output')\n    \n    filename = \"/kaggle/working/DPT/run_monodepth.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"output_monodepth\"', '\"/kaggle/working/DPT/output_monodepth\"'))\n\n    filename = \"/kaggle/working/DPT/run_segmentation.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"output_semseg\"', '\"/kaggle/working/DPT/output_semseg\"'))\n    \n    filename = \"/kaggle/working/DPT/run_monodepth.py\"\n    text = open(filename).read()\n\n    #Here goes your files\n    open(filename, \"w+\").write(text.replace('\"input\"', '\"/kaggle/working/DPT/input/\"'))\n\n    filename = \"/kaggle/working/DPT/run_segmentation.py\"\n    text = open(filename).read()\n\n    #Here goes your files\n    open(filename, \"w+\").write(text.replace('\"input\"', '\"/kaggle/working/DPT/input/\"'))\n    \n    filename = \"/kaggle/working/DPT/run_monodepth.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"weights/', '\"/kaggle/working/DPT/weights/'))\n\n    filename = \"/kaggle/working/DPT/run_segmentation.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"weights/', '\"/kaggle/working/DPT/weights/'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.552080Z","iopub.execute_input":"2025-06-14T13:52:25.552328Z","iopub.status.idle":"2025-06-14T13:52:25.558421Z","shell.execute_reply.started":"2025-06-14T13:52:25.552292Z","shell.execute_reply":"2025-06-14T13:52:25.557825Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"initialize_system()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.560761Z","iopub.execute_input":"2025-06-14T13:52:25.560947Z","iopub.status.idle":"2025-06-14T13:52:25.572495Z","shell.execute_reply.started":"2025-06-14T13:52:25.560932Z","shell.execute_reply":"2025-06-14T13:52:25.571954Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Training parameters\ndropout = 0.25\nactivation = \"ReLU\"\nweight_decay = 0.0\nearly_stopping = False\nlearning_rate = 1e-4\nbatch_size = 4\nepochs = 15\noptimizer_selected = \"Adam\"\nscheduler_selected = \"StepLR\"\ncriterion_selected = \"CrossEntropy\"\nDATASET_PATH = \"../input/dataset-v2/d2/\"#\"../input/dataset-v2/d2/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.573154Z","iopub.execute_input":"2025-06-14T13:52:25.573376Z","iopub.status.idle":"2025-06-14T13:52:25.582485Z","shell.execute_reply.started":"2025-06-14T13:52:25.573354Z","shell.execute_reply":"2025-06-14T13:52:25.581816Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"with open(\"DPT/dpt/models.py\", \"r\") as f:\n    code = f.read()\n# Paso 1: Reemplazar la línea de Dropout fija por una variable\ncode = code.replace(\"nn.Dropout(0.1, False)\", \"dropout_layer\")\n#code = code.replace(\"dropout_layer,\", \"dropout_layer\")\nif activation == \"SiLu\":\n    code = code.replace(\n        \"            nn.BatchNorm2d(features),\\n            nn.ReLU(True),\",\n        \"            nn.BatchNorm2d(features),\\n            nn.SiLU(True),\"\n    )\nelif activation == \"GELU\":\n    code = code.replace(\n        \"            nn.BatchNorm2d(features),\\n            nn.ReLU(True),\",\n        \"            nn.BatchNorm2d(features),\\n            nn.GELU(),\"\n    )\n# Paso 3: Asegurar que el constructor tiene los nuevos argumentos\nif \"dropout_rate\" not in code:\n    code = code.replace(\n        \"def __init__(self, num_classes, path=None, **kwargs):\",\n        \"def __init__(self, num_classes, path=None, dropout_rate=0.1, dropout_inplace=False, **kwargs):\\n        dropout_layer = nn.Dropout(dropout_rate, inplace=dropout_inplace)\\n\"\n    )\n\nwith open(\"DPT/dpt/models.py\", \"w\") as f:\n    f.write(code)\n\n!cat DPT/dpt/models.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.583116Z","iopub.execute_input":"2025-06-14T13:52:25.583269Z","iopub.status.idle":"2025-06-14T13:52:25.708422Z","shell.execute_reply.started":"2025-06-14T13:52:25.583256Z","shell.execute_reply":"2025-06-14T13:52:25.707600Z"}},"outputs":[{"name":"stdout","text":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_model import BaseModel\nfrom .blocks import (\n    FeatureFusionBlock,\n    FeatureFusionBlock_custom,\n    Interpolate,\n    _make_encoder,\n    forward_vit,\n)\n\n\ndef _make_fusion_block(features, use_bn):\n    return FeatureFusionBlock_custom(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,\n        align_corners=True,\n    )\n\n\nclass DPT(BaseModel):\n    def __init__(\n        self,\n        head,\n        features=256,\n        backbone=\"vitb_rn50_384\",\n        readout=\"project\",\n        channels_last=False,\n        use_bn=False,\n        enable_attention_hooks=False,\n    ):\n\n        super(DPT, self).__init__()\n\n        self.channels_last = channels_last\n\n        hooks = {\n            \"vitb_rn50_384\": [0, 1, 8, 11],\n            \"vitb16_384\": [2, 5, 8, 11],\n            \"vitl16_384\": [5, 11, 17, 23],\n        }\n\n        # Instantiate backbone and reassemble blocks\n        self.pretrained, self.scratch = _make_encoder(\n            backbone,\n            features,\n            False,  # Set to true of you want to train from scratch, uses ImageNet weights\n            groups=1,\n            expand=False,\n            exportable=False,\n            hooks=hooks[backbone],\n            use_readout=readout,\n            enable_attention_hooks=enable_attention_hooks,\n        )\n\n        self.scratch.refinenet1 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet2 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet3 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet4 = _make_fusion_block(features, use_bn)\n\n        self.scratch.output_conv = head\n\n    def forward(self, x):\n        if self.channels_last == True:\n            x.contiguous(memory_format=torch.channels_last)\n\n        layer_1, layer_2, layer_3, layer_4 = forward_vit(self.pretrained, x)\n\n        layer_1_rn = self.scratch.layer1_rn(layer_1)\n        layer_2_rn = self.scratch.layer2_rn(layer_2)\n        layer_3_rn = self.scratch.layer3_rn(layer_3)\n        layer_4_rn = self.scratch.layer4_rn(layer_4)\n\n        path_4 = self.scratch.refinenet4(layer_4_rn)\n        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n\n        out = self.scratch.output_conv(path_1)\n\n        return out\n\n\nclass DPTDepthModel(DPT):\n    def __init__(\n        self, path=None, non_negative=True, scale=1.0, shift=0.0, invert=False, **kwargs\n    ):\n        features = kwargs[\"features\"] if \"features\" in kwargs else 256\n\n        self.scale = scale\n        self.shift = shift\n        self.invert = invert\n\n        head = nn.Sequential(\n            nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(True) if non_negative else nn.Identity(),\n            nn.Identity(),\n        )\n\n        super().__init__(head, **kwargs)\n\n        if path is not None:\n            self.load(path)\n\n    def forward(self, x):\n        inv_depth = super().forward(x).squeeze(dim=1)\n\n        if self.invert:\n            depth = self.scale * inv_depth + self.shift\n            depth[depth < 1e-8] = 1e-8\n            depth = 1.0 / depth\n            return depth\n        else:\n            return inv_depth\n\n\nclass DPTSegmentationModel(DPT):\n    def __init__(self, num_classes, path=None, dropout_rate=0.1, dropout_inplace=False, **kwargs):\n        dropout_layer = nn.Dropout(dropout_rate, inplace=dropout_inplace)\n\n\n        features = kwargs[\"features\"] if \"features\" in kwargs else 256\n\n        kwargs[\"use_bn\"] = True\n\n        head = nn.Sequential(\n            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(True),\n            dropout_layer,\n            nn.Conv2d(features, num_classes, kernel_size=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n        )\n\n        super().__init__(head, **kwargs)\n\n        self.auxlayer = nn.Sequential(\n            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(True),\n            dropout_layer,\n            nn.Conv2d(features, num_classes, kernel_size=1),\n        )\n\n        if path is not None:\n            self.load(path)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"with open(\"DPT/dpt/base_model.py\", \"r\") as f:\n    code = f.read()\n\ncode = code.replace(\n    \"self.load_state_dict(parameters)\",\n    \"\"\"own_state = self.state_dict()\n        filtered = {k: v for k, v in parameters.items() if k in own_state and v.shape == own_state[k].shape}\n        print(f\"Cargando {len(filtered)} de {len(own_state)} parámetros del checkpoint.\")\n        self.load_state_dict(filtered, strict=False)\"\"\"\n)\n\n\nwith open(\"DPT/dpt/base_model.py\", \"w\") as f:\n    f.write(code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.709610Z","iopub.execute_input":"2025-06-14T13:52:25.709876Z","iopub.status.idle":"2025-06-14T13:52:25.715080Z","shell.execute_reply.started":"2025-06-14T13:52:25.709844Z","shell.execute_reply":"2025-06-14T13:52:25.714543Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Leer contenido original\nwith open(\"DPT/dpt/models.py\", \"r\") as f:\n    code = f.read()\n\n# Añadir el nuevo método seguro al final del archivo si no existe\nif \"def load_partial_weights\" not in code:\n    code += \"\"\"\n\n    def load_partial_weights(self, path):\n        parameters = torch.load(path, map_location=\"cpu\")\n        if \"model\" in parameters:\n            parameters = parameters[\"model\"]\n\n        own_state = self.state_dict()\n        filtered = {\n            k: v for k, v in parameters.items()\n            if k in own_state and v.shape == own_state[k].shape\n        }\n\n        print(f\"Cargando {len(filtered)} de {len(own_state)} parámetros del checkpoint.\")\n        self.load_state_dict(filtered, strict=False)\n\"\"\"\n\n\n\n# Guardar cambios\nwith open(\"DPT/dpt/models.py\", \"w\") as f:\n    f.write(code)\n\nprint(\"Archivo 'models.py' modificado exitosamente.\")\n\n\n!cat DPT/dpt/models.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.715938Z","iopub.execute_input":"2025-06-14T13:52:25.716190Z","iopub.status.idle":"2025-06-14T13:52:25.839560Z","shell.execute_reply.started":"2025-06-14T13:52:25.716168Z","shell.execute_reply":"2025-06-14T13:52:25.838943Z"}},"outputs":[{"name":"stdout","text":"Archivo 'models.py' modificado exitosamente.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_model import BaseModel\nfrom .blocks import (\n    FeatureFusionBlock,\n    FeatureFusionBlock_custom,\n    Interpolate,\n    _make_encoder,\n    forward_vit,\n)\n\n\ndef _make_fusion_block(features, use_bn):\n    return FeatureFusionBlock_custom(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,\n        align_corners=True,\n    )\n\n\nclass DPT(BaseModel):\n    def __init__(\n        self,\n        head,\n        features=256,\n        backbone=\"vitb_rn50_384\",\n        readout=\"project\",\n        channels_last=False,\n        use_bn=False,\n        enable_attention_hooks=False,\n    ):\n\n        super(DPT, self).__init__()\n\n        self.channels_last = channels_last\n\n        hooks = {\n            \"vitb_rn50_384\": [0, 1, 8, 11],\n            \"vitb16_384\": [2, 5, 8, 11],\n            \"vitl16_384\": [5, 11, 17, 23],\n        }\n\n        # Instantiate backbone and reassemble blocks\n        self.pretrained, self.scratch = _make_encoder(\n            backbone,\n            features,\n            False,  # Set to true of you want to train from scratch, uses ImageNet weights\n            groups=1,\n            expand=False,\n            exportable=False,\n            hooks=hooks[backbone],\n            use_readout=readout,\n            enable_attention_hooks=enable_attention_hooks,\n        )\n\n        self.scratch.refinenet1 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet2 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet3 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet4 = _make_fusion_block(features, use_bn)\n\n        self.scratch.output_conv = head\n\n    def forward(self, x):\n        if self.channels_last == True:\n            x.contiguous(memory_format=torch.channels_last)\n\n        layer_1, layer_2, layer_3, layer_4 = forward_vit(self.pretrained, x)\n\n        layer_1_rn = self.scratch.layer1_rn(layer_1)\n        layer_2_rn = self.scratch.layer2_rn(layer_2)\n        layer_3_rn = self.scratch.layer3_rn(layer_3)\n        layer_4_rn = self.scratch.layer4_rn(layer_4)\n\n        path_4 = self.scratch.refinenet4(layer_4_rn)\n        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n\n        out = self.scratch.output_conv(path_1)\n\n        return out\n\n\nclass DPTDepthModel(DPT):\n    def __init__(\n        self, path=None, non_negative=True, scale=1.0, shift=0.0, invert=False, **kwargs\n    ):\n        features = kwargs[\"features\"] if \"features\" in kwargs else 256\n\n        self.scale = scale\n        self.shift = shift\n        self.invert = invert\n\n        head = nn.Sequential(\n            nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(True) if non_negative else nn.Identity(),\n            nn.Identity(),\n        )\n\n        super().__init__(head, **kwargs)\n\n        if path is not None:\n            self.load(path)\n\n    def forward(self, x):\n        inv_depth = super().forward(x).squeeze(dim=1)\n\n        if self.invert:\n            depth = self.scale * inv_depth + self.shift\n            depth[depth < 1e-8] = 1e-8\n            depth = 1.0 / depth\n            return depth\n        else:\n            return inv_depth\n\n\nclass DPTSegmentationModel(DPT):\n    def __init__(self, num_classes, path=None, dropout_rate=0.1, dropout_inplace=False, **kwargs):\n        dropout_layer = nn.Dropout(dropout_rate, inplace=dropout_inplace)\n\n\n        features = kwargs[\"features\"] if \"features\" in kwargs else 256\n\n        kwargs[\"use_bn\"] = True\n\n        head = nn.Sequential(\n            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(True),\n            dropout_layer,\n            nn.Conv2d(features, num_classes, kernel_size=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n        )\n\n        super().__init__(head, **kwargs)\n\n        self.auxlayer = nn.Sequential(\n            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(True),\n            dropout_layer,\n            nn.Conv2d(features, num_classes, kernel_size=1),\n        )\n\n        if path is not None:\n            self.load(path)\n\n\n    def load_partial_weights(self, path):\n        parameters = torch.load(path, map_location=\"cpu\")\n        if \"model\" in parameters:\n            parameters = parameters[\"model\"]\n\n        own_state = self.state_dict()\n        filtered = {\n            k: v for k, v in parameters.items()\n            if k in own_state and v.shape == own_state[k].shape\n        }\n\n        print(f\"Cargando {len(filtered)} de {len(own_state)} parámetros del checkpoint.\")\n        self.load_state_dict(filtered, strict=False)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!cat DPT/dpt/base_model.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.840560Z","iopub.execute_input":"2025-06-14T13:52:25.840829Z","iopub.status.idle":"2025-06-14T13:52:25.955884Z","shell.execute_reply.started":"2025-06-14T13:52:25.840798Z","shell.execute_reply":"2025-06-14T13:52:25.955293Z"}},"outputs":[{"name":"stdout","text":"import torch\n\n\nclass BaseModel(torch.nn.Module):\n    def load(self, path):\n        \"\"\"Load model from file.\n\n        Args:\n            path (str): file path\n        \"\"\"\n        parameters = torch.load(path, map_location=torch.device(\"cpu\"))\n\n        if \"optimizer\" in parameters:\n            parameters = parameters[\"model\"]\n\n        own_state = self.state_dict()\n        filtered = {k: v for k, v in parameters.items() if k in own_state and v.shape == own_state[k].shape}\n        print(f\"Cargando {len(filtered)} de {len(own_state)} parámetros del checkpoint.\")\n        self.load_state_dict(filtered, strict=False)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ===============================\n# IMPORTS\n# ===============================\nimport os\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader,random_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.metrics import f1_score\nfrom DPT.dpt.models import DPTSegmentationModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:25.956717Z","iopub.execute_input":"2025-06-14T13:52:25.956887Z","iopub.status.idle":"2025-06-14T13:52:36.721290Z","shell.execute_reply.started":"2025-06-14T13:52:25.956867Z","shell.execute_reply":"2025-06-14T13:52:36.720728Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ===============================\n# DATASET CON ALBUMENTATIONS\n# ===============================\nclass RoadSignSegmentationDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.samples = []\n        self.transform = transform\n\n        subdirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir)\n                   if os.path.isdir(os.path.join(root_dir, d))]\n\n        for subdir in subdirs:\n            for fname in os.listdir(subdir):\n                if fname.endswith(\"_img.png\"):\n                    base = fname.replace(\"_img.png\", \"\")\n                    img_path = os.path.join(subdir, f\"{base}_img.png\")\n                    mask_path = os.path.join(subdir, f\"{base}_label.png\")\n                    label_names_path = os.path.join(subdir, f\"{base}_label_names.txt\")\n\n                    if os.path.exists(mask_path):\n                        label_names = []\n                        if os.path.exists(label_names_path):\n                            with open(label_names_path, 'r') as f:\n                                label_names = [line.strip() for line in f.readlines()]\n\n                        self.samples.append({\n                            \"image\": img_path,\n                            \"mask\": mask_path,\n                            \"labels\": label_names\n                        })\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        image = np.array(Image.open(sample[\"image\"]).convert(\"RGB\"))\n        mask = np.array(Image.open(sample[\"mask\"]))\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented[\"image\"]\n            mask = augmented[\"mask\"].long()\n\n        return image, mask, sample[\"labels\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:36.721882Z","iopub.execute_input":"2025-06-14T13:52:36.722208Z","iopub.status.idle":"2025-06-14T13:52:36.729265Z","shell.execute_reply.started":"2025-06-14T13:52:36.722183Z","shell.execute_reply":"2025-06-14T13:52:36.728541Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ===============================\n# TRANSFORMACIONES\n# ===============================\ntrain_transform = A.Compose([\n    A.Resize(512, 512),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.GaussNoise(p=0.2),\n    A.Affine(\n        scale=(0.8, 1.2),\n        translate_percent={\"x\": 0.05, \"y\": 0.05},\n        rotate=(-15, 15),\n        p=0.5\n    ),\n    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ToTensorV2()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:36.730102Z","iopub.execute_input":"2025-06-14T13:52:36.730477Z","iopub.status.idle":"2025-06-14T13:52:36.748258Z","shell.execute_reply.started":"2025-06-14T13:52:36.730458Z","shell.execute_reply":"2025-06-14T13:52:36.747651Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ===============================\n# MÉTRICAS\n# ===============================\ndef compute_iou(pred, target, num_classes=2):\n    ious = []\n    for cls in range(num_classes):\n        pred_inds = pred == cls\n        target_inds = target == cls\n        intersection = (pred_inds & target_inds).sum()\n        union = (pred_inds | target_inds).sum()\n        if union == 0:\n            ious.append(np.nan)\n        else:\n            ious.append(intersection / union)\n    return np.nanmean(ious)\n\ndef pixel_accuracy(pred, target):\n    correct = (pred == target).sum()\n    total = target.size\n    return correct / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:36.749019Z","iopub.execute_input":"2025-06-14T13:52:36.749446Z","iopub.status.idle":"2025-06-14T13:52:36.757099Z","shell.execute_reply.started":"2025-06-14T13:52:36.749422Z","shell.execute_reply":"2025-06-14T13:52:36.756436Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ===============================\n# DATASET\n# ===============================\ndataset = RoadSignSegmentationDataset(\n    root_dir=DATASET_PATH,\n    transform=train_transform\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:36.757947Z","iopub.execute_input":"2025-06-14T13:52:36.758218Z","iopub.status.idle":"2025-06-14T13:52:39.575594Z","shell.execute_reply.started":"2025-06-14T13:52:36.758195Z","shell.execute_reply":"2025-06-14T13:52:39.574830Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"for image, mask, labels in dataset:\n    print(labels)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:39.576478Z","iopub.execute_input":"2025-06-14T13:52:39.576738Z","iopub.status.idle":"2025-06-14T13:52:40.309128Z","shell.execute_reply.started":"2025-06-14T13:52:39.576712Z","shell.execute_reply":"2025-06-14T13:52:40.308323Z"}},"outputs":[{"name":"stdout","text":"['_background_', 'prevention']\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ===============================\n# CONFIGURACIÓN\n# ===============================\nNUM_CLASSES = 4\nBATCH_SIZE = batch_size\nNUM_EPOCHS = epochs\nLEARNING_RATE = 1e-5\nVAL_SPLIT = 0.2\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ===============================\n# DATALOADER\n# ===============================\nval_size = int(VAL_SPLIT * len(dataset))\ntrain_size = len(dataset) - val_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(len(train_dataset))\nprint(len(val_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:40.309908Z","iopub.execute_input":"2025-06-14T13:52:40.310260Z","iopub.status.idle":"2025-06-14T13:52:42.047663Z","shell.execute_reply.started":"2025-06-14T13:52:40.310219Z","shell.execute_reply":"2025-06-14T13:52:42.046842Z"}},"outputs":[{"name":"stdout","text":"193\n48\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ===============================\n# MODELO\n# ===============================\nmodel = DPTSegmentationModel(\n    num_classes=NUM_CLASSES,\n    dropout_rate=dropout,\n    backbone=\"vitl16_384\",\n    readout=\"project\",\n    features=256,\n    use_bn=True\n)\n\nmodel.load_partial_weights(\"DPT/weights/dpt_large-ade20k-b12dca68.pt\")\nmodel.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:42.048565Z","iopub.execute_input":"2025-06-14T13:52:42.048825Z","iopub.status.idle":"2025-06-14T13:52:49.119769Z","shell.execute_reply.started":"2025-06-14T13:52:42.048804Z","shell.execute_reply":"2025-06-14T13:52:49.119109Z"}},"outputs":[{"name":"stdout","text":"Cargando 438 de 442 parámetros del checkpoint.\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"DPTSegmentationModel(\n  (pretrained): Module(\n    (model): VisionTransformer(\n      (patch_embed): PatchEmbed(\n        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n        (norm): Identity()\n      )\n      (pos_drop): Dropout(p=0.0, inplace=False)\n      (patch_drop): Identity()\n      (norm_pre): Identity()\n      (blocks): Sequential(\n        (0): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (1): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (2): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (3): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (4): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (5): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (6): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (7): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (8): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (9): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (10): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (11): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (12): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (13): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (14): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (15): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (16): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (17): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (18): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (19): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (20): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (21): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (22): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (23): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n      )\n      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n      (fc_norm): Identity()\n      (head_drop): Dropout(p=0.0, inplace=False)\n      (head): Linear(in_features=1024, out_features=1000, bias=True)\n    )\n    (act_postprocess1): Sequential(\n      (0): ProjectReadout(\n        (project): Sequential(\n          (0): Linear(in_features=2048, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n        )\n      )\n      (1): Transpose()\n      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n      (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n      (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n    )\n    (act_postprocess2): Sequential(\n      (0): ProjectReadout(\n        (project): Sequential(\n          (0): Linear(in_features=2048, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n        )\n      )\n      (1): Transpose()\n      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n      (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (act_postprocess3): Sequential(\n      (0): ProjectReadout(\n        (project): Sequential(\n          (0): Linear(in_features=2048, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n        )\n      )\n      (1): Transpose()\n      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (act_postprocess4): Sequential(\n      (0): ProjectReadout(\n        (project): Sequential(\n          (0): Linear(in_features=2048, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n        )\n      )\n      (1): Transpose()\n      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n  )\n  (scratch): Module(\n    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (refinenet1): FeatureFusionBlock_custom(\n      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (resConfUnit1): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (resConfUnit2): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n    )\n    (refinenet2): FeatureFusionBlock_custom(\n      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (resConfUnit1): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (resConfUnit2): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n    )\n    (refinenet3): FeatureFusionBlock_custom(\n      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (resConfUnit1): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (resConfUnit2): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n    )\n    (refinenet4): FeatureFusionBlock_custom(\n      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (resConfUnit1): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (resConfUnit2): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n    )\n    (output_conv): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Dropout(p=0.25, inplace=False)\n      (4): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n      (5): Interpolate()\n    )\n  )\n  (auxlayer): Sequential(\n    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n  )\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# ===============================\n# CONGELAR BACKBONE\n# ===============================\nfor param in model.pretrained.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:49.120568Z","iopub.execute_input":"2025-06-14T13:52:49.120835Z","iopub.status.idle":"2025-06-14T13:52:49.125434Z","shell.execute_reply.started":"2025-06-14T13:52:49.120810Z","shell.execute_reply":"2025-06-14T13:52:49.124764Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:49.128791Z","iopub.execute_input":"2025-06-14T13:52:49.129184Z","iopub.status.idle":"2025-06-14T13:52:49.865509Z","shell.execute_reply.started":"2025-06-14T13:52:49.129167Z","shell.execute_reply":"2025-06-14T13:52:49.864668Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!mkdir ./checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:49.866274Z","iopub.execute_input":"2025-06-14T13:52:49.866467Z","iopub.status.idle":"2025-06-14T13:52:50.041278Z","shell.execute_reply.started":"2025-06-14T13:52:49.866450Z","shell.execute_reply":"2025-06-14T13:52:50.040478Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import time  # LINEA NUEVA\nimport pandas as pd  # LINEA NUEVA\nimport GPUtil  # LINEA NUEVA\nfrom sklearn.metrics import precision_score, recall_score  # LINEA NUEVA\n\n# ===============================\n# ENTRENAMIENTO (CONGELADO)\n# ===============================\nif criterion_selected == \"CrossEntropySmoothing\":\n    criterion = CrossEntropySmoothing(label_smoothing=0.1)\nelif criterion_selected == \"CrossEntropy\":\n    criterion = nn.CrossEntropyLoss()\n\nif optimizer_selected == \"Adam\":\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)\nelif optimizer_selected == \"AdamW\":\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)\n\nif scheduler_selected == \"StepLR\":\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\nelif scheduler_selected == \"ReduceLROnPlateau\":\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\nelif scheduler_selected == \"CosineAnnealingLR\":\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n\nmetrics = {  # LINEA NUEVA\n    \"epoch\": [],  # LINEA NUEVA\n    \"train_loss\": [], \"train_iou\": [], \"train_precision\": [], \"train_recall\": [], \"train_f1\": [], \"train_acc\": [],  # LINEA NUEVA\n    \"val_loss\": [], \"val_iou\": [], \"val_precision\": [], \"val_recall\": [], \"val_f1\": [], \"val_acc\": [],  # LINEA NUEVA\n    \"lr\": [], \"epoch_time\": [], \"gpu_mem_mb\": []  # LINEA NUEVA\n}\n\npatience = 10\npatience_counter = 0\nbest_val_loss = float(\"inf\")\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n    start_time = time.time()  # LINEA NUEVA\n    model.train()\n    running_loss = 0.0\n    train_preds, train_targets = [], []  # LINEA NUEVA\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}\"):\n        images, masks = images.to(DEVICE), masks.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n        preds = torch.argmax(outputs, dim=1)\n        train_preds.extend(preds.cpu().numpy())  # LINEA NUEVA\n        train_targets.extend(masks.cpu().numpy())  # LINEA NUEVA\n\n    # TRAIN METRICS\n    train_preds_flat = np.array(train_preds).flatten()  # LINEA NUEVA\n    train_targets_flat = np.array(train_targets).flatten()  # LINEA NUEVA\n    train_iou = compute_iou(np.array(train_preds), np.array(train_targets))  # LINEA NUEVA\n    train_acc = pixel_accuracy(np.array(train_preds), np.array(train_targets))  # LINEA NUEVA\n    train_precision = precision_score(train_targets_flat, train_preds_flat, average=\"macro\", zero_division=0)  # LINEA NUEVA\n    train_recall = recall_score(train_targets_flat, train_preds_flat, average=\"macro\", zero_division=0)  # LINEA NUEVA\n    train_f1 = f1_score(train_targets_flat, train_preds_flat, average=\"macro\", zero_division=0)  # LINEA NUEVA\n\n    # VALIDATION\n    model.eval()\n    val_loss = 0.0\n    val_preds, val_targets = [], []\n\n    with torch.no_grad():\n        for images, masks, _ in val_loader:\n            images, masks = images.to(DEVICE), masks.to(DEVICE)\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            val_loss += loss.item()\n\n            preds = torch.argmax(outputs, dim=1)\n            val_preds.extend(preds.cpu().numpy())\n            val_targets.extend(masks.cpu().numpy())\n\n    val_preds_flat = np.array(val_preds).flatten()\n    val_targets_flat = np.array(val_targets).flatten()\n    val_iou = compute_iou(np.array(val_preds), np.array(val_targets))\n    val_acc = pixel_accuracy(np.array(val_preds), np.array(val_targets))\n    val_precision = precision_score(val_targets_flat, val_preds_flat, average=\"macro\", zero_division=0)\n    val_recall = recall_score(val_targets_flat, val_preds_flat, average=\"macro\", zero_division=0)\n    val_f1 = f1_score(val_targets_flat, val_preds_flat, average=\"macro\", zero_division=0)\n\n    # GPU + Tiempo\n    epoch_time = time.time() - start_time  # LINEA NUEVA\n    gpu_mem = GPUtil.getGPUs()[0].memoryUsed if torch.cuda.is_available() else 0  # LINEA NUEVA\n\n    # Log CSV\n    metrics[\"epoch\"].append(epoch)  # LINEA NUEVA\n    metrics[\"train_loss\"].append(running_loss / len(train_loader))\n    metrics[\"train_iou\"].append(train_iou)\n    metrics[\"train_precision\"].append(train_precision)\n    metrics[\"train_recall\"].append(train_recall)\n    metrics[\"train_f1\"].append(train_f1)\n    metrics[\"train_acc\"].append(train_acc)\n    metrics[\"val_loss\"].append(val_loss / len(val_loader))\n    metrics[\"val_iou\"].append(val_iou)\n    metrics[\"val_precision\"].append(val_precision)\n    metrics[\"val_recall\"].append(val_recall)\n    metrics[\"val_f1\"].append(val_f1)\n    metrics[\"val_acc\"].append(val_acc)\n    metrics[\"lr\"].append(optimizer.param_groups[0][\"lr\"])  # LINEA NUEVA\n    metrics[\"epoch_time\"].append(epoch_time)  # LINEA NUEVA\n    metrics[\"gpu_mem_mb\"].append(gpu_mem)  # LINEA NUEVA\n\n    # Guardar CSV\n    pd.DataFrame(metrics).to_csv(\"checkpoints/metrics_log.csv\", index=False)  # LINEA NUEVA\n\n    train_loss = metrics[\"train_loss\"][-1]\n    val_loss = metrics[\"val_loss\"][-1]\n    print(f\"[Train] Epoch {epoch+1} | Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n    print(f\"[Val]   Epoch {epoch+1} | Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n\n    scheduler.step(val_loss if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) else None)\n\n    # EARLY STOPPING\n    if early_stopping:\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), \"checkpoints/best_model.pt\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"⏹️ Early stopping triggered at epoch {epoch+1}\")\n                break\n\n# Cargar mejor modelo\nif early_stopping:\n    model.load_state_dict(torch.load(\"checkpoints/best_model.pt\"))\n    print(\"✅ Modelo óptimo cargado desde early stopping.\")\n\n# ===============================\n# GUARDAR MODELO\n# ===============================\nos.makedirs(\"checkpoints\", exist_ok=True)\ntorch.save(model.state_dict(), \"checkpoints/dpt_finetuned.pt\")\nprint(\"Modelo guardado en checkpoints/dpt_finetuned.pt\")\n\n# ===============================\n# GRÁFICAS\n# ===============================\nplt.figure(figsize=(12, 8))\nplt.plot(metrics[\"train_loss\"], label=\"Train Loss\")\nplt.plot(metrics[\"val_loss\"], label=\"Val Loss\")\nplt.plot(metrics[\"train_iou\"], label=\"Train IoU\")\nplt.plot(metrics[\"val_iou\"], label=\"Val IoU\")\nplt.plot(metrics[\"train_acc\"], label=\"Train Accuracy\")\nplt.plot(metrics[\"val_acc\"], label=\"Val Accuracy\")\nplt.plot(metrics[\"train_f1\"], label=\"Train F1\")\nplt.plot(metrics[\"val_f1\"], label=\"Val F1\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Valor\")\nplt.title(\"Métricas de Entrenamiento y Validación\")\nplt.grid(True)\nplt.legend()\nplt.savefig(\"checkpoints/metrics_plot.png\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:52:50.042335Z","iopub.execute_input":"2025-06-14T13:52:50.042614Z"}},"outputs":[{"name":"stdout","text":"Epoch 0/15\n","output_type":"stream"},{"name":"stderr","text":"[Train] Epoch 1:  10%|█         | 5/49 [00:15<02:15,  3.07s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd  # Asegurate de tenerlo ya importado\nimport matplotlib.pyplot as plt\nimport os\n\n# LINEA NUEVA: Leer el CSV exportado\nmetrics = pd.read_csv(\"checkpoints/metrics_log.csv\")  # LINEA NUEVA\n\n# LINEA NUEVA: Convertir a listas si es necesario\nmetrics = {col: metrics[col].tolist() for col in metrics.columns}  # LINEA NUEVA\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n\n# Asegura carpeta de salida\nos.makedirs(\"checkpoints/metric_plots\", exist_ok=True)\n\ndef plot_metric(train_values, val_values, title, ylabel, filename):\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_values, label=\"Train\")\n    plt.plot(val_values, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(f\"checkpoints/metric_plots/{filename}.png\")\n    plt.show()\n\n# 1. Pérdida (Loss) para detectar overfitting/underfitting\nplot_metric(\n    metrics[\"train_loss\"],\n    metrics[\"val_loss\"],\n    \"Evolución de la Pérdida\",\n    \"CrossEntropy Loss\",\n    \"loss_comparison\"\n)\n\n# 2. IoU (Intersección sobre Unión) - Métrica clave para segmentación\nplot_metric(\n    metrics[\"train_iou\"],\n    metrics[\"val_iou\"],\n    \"Evolución del IoU\",\n    \"IoU\",\n    \"iou_comparison\"\n)\n\n# 3. Pixel Accuracy - Proporción de píxeles correctamente clasificados\nplot_metric(\n    metrics[\"train_acc\"],\n    metrics[\"val_acc\"],\n    \"Exactitud por píxel\",\n    \"Pixel Accuracy\",\n    \"accuracy_comparison\"\n)\n\n# 4. F1 Score - Balance entre precisión y recall\nplot_metric(\n    metrics[\"train_f1\"],\n    metrics[\"val_f1\"],\n    \"Evolución del F1 Score \",\n    \"F1 Score\",\n    \"f1_comparison\"\n)\n# 5. Precision\nplot_metric(\n    metrics[\"train_precision\"],\n    metrics[\"val_precision\"],\n    \"Precisión del modelo\",\n    \"Precision\",\n    \"precision_comparison\"\n)\n\n# 6. Recall\nplot_metric(\n    metrics[\"train_recall\"],\n    metrics[\"val_recall\"],\n    \"Recall del modelo\",\n    \"Recall\",\n    \"recall_comparison\"\n)\n\n# 7. Tiempo por época\nplt.figure(figsize=(10, 6))\nplt.plot(metrics[\"epoch_time\"], label=\"Duración por época\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Segundos\")\nplt.title(\"Duración por época\")\nplt.grid(True)\nplt.savefig(\"checkpoints/metric_plots/epoch_time.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport os\n\n# Cargar métricas desde CSV\nmetrics_path = \"checkpoints/metrics_log.csv\"\nmetrics_df = pd.read_csv(metrics_path)\n\n# Crear carpeta de salida\nos.makedirs(\"checkpoints/metric_plots\", exist_ok=True)\n\n# Función para graficar y agregar al PDF\ndef plot_metric_to_pdf(pdf, x, train_values, val_values, title, ylabel):\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, train_values, label=\"Train\")\n    plt.plot(x, val_values, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    pdf.savefig()\n    plt.close()\n\n# Crear PDF\npdf_path = \"checkpoints/metric_plots/metric_report.pdf\"\nwith PdfPages(pdf_path) as pdf:\n    x = metrics_df[\"epoch\"]\n\n    plot_metric_to_pdf(pdf, x, metrics_df[\"train_loss\"], metrics_df[\"val_loss\"], \"Evolución de la Pérdida\", \"CrossEntropy Loss\")\n    plot_metric_to_pdf(pdf, x, metrics_df[\"train_iou\"], metrics_df[\"val_iou\"], \"Evolución del IoU\", \"IoU\")\n    plot_metric_to_pdf(pdf, x, metrics_df[\"train_acc\"], metrics_df[\"val_acc\"], \"Exactitud por píxel\", \"Pixel Accuracy\")\n    plot_metric_to_pdf(pdf, x, metrics_df[\"train_f1\"], metrics_df[\"val_f1\"], \"Evolución del F1 Score\", \"F1 Score\")\n    plot_metric_to_pdf(pdf, x, metrics_df[\"train_precision\"], metrics_df[\"val_precision\"], \"Precisión del modelo\", \"Precision\")\n    plot_metric_to_pdf(pdf, x, metrics_df[\"train_recall\"], metrics_df[\"val_recall\"], \"Recall del modelo\", \"Recall\")\n    \n    # Plot tiempo por época\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, metrics_df[\"epoch_time\"], label=\"Duración por época\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Segundos\")\n    plt.title(\"Duración por época\")\n    plt.grid(True)\n    pdf.savefig()\n    plt.close()\n\npdf_path\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp checkpoints/metric_plots/metric_report.pdf ./metric_report.pdf\n!cp checkpoints/metrics_log.csv ./metrics_log.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(model, dataloader, device, num_classes=2, normalize=True, title=\"Matriz de Confusión\"):\n    all_preds = []\n    all_targets = []\n\n    model.eval()\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy().flatten())\n            all_targets.extend(masks.cpu().numpy().flatten())\n\n    cm = confusion_matrix(all_targets, all_preds, labels=list(range(num_classes)), normalize='true' if normalize else None)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f\"Clase {i}\" for i in range(num_classes)])\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.grid(False)\n    plt.savefig(\"checkpoints/confusion_matrix.png\")\n    plt.show()\n\nplot_confusion_matrix(model, val_loader, DEVICE, num_classes=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# EVALUADOR DE MODELO\n# ===============================\ndef evaluate_model(model_path, dataloader, device, num_classes=2, pretrained=True):\n    if pretrained:\n        model = DPTSegmentationModel(\n            num_classes=num_classes,\n            path=model_path,\n            backbone=\"vitl16_384\",\n        )\n    else:\n        model = DPTSegmentationModel(\n            num_classes=num_classes,\n            path=None,\n            backbone=\"vitl16_384\",\n        )\n        model.load_partial_weights(model_path)\n\n    model.to(device)\n    model.eval()\n\n    iou_scores = []\n    pixel_accuracies = []\n    f1_scores = []\n\n    with torch.no_grad():\n        for images, masks, labels in tqdm(dataloader, desc=f\"Evaluando {model_path}\"):\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n\n            for p, t in zip(preds, masks):\n                p_np = p.cpu().numpy()\n                t_np = t.cpu().numpy()\n                iou_scores.append(compute_iou(p_np, t_np, num_classes))\n                pixel_accuracies.append(pixel_accuracy(p_np, t_np))\n                f1_scores.append(f1_score(t_np.flatten(), p_np.flatten(), average=\"macro\"))\n\n    return {\n        \"IoU\": np.nanmean(iou_scores),\n        \"Pixel Accuracy\": np.mean(pixel_accuracies),\n        \"F1 Score\": np.mean(f1_scores)\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_transform = A.Compose([\n    A.Resize(512, 512),\n    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ToTensorV2()\n])\n\neval_dataset = RoadSignSegmentationDataset(DATASET_PATH, transform=eval_transform)\neval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Evaluación\nprint(\"Evaluando modelo preentrenado...\")\nresults_pre = evaluate_model(\"DPT/weights/dpt_large-ade20k-b12dca68.pt\", eval_loader, DEVICE, NUM_CLASSES, pretrained=True)\nprint(\"Modelo Preentrenado:\", results_pre)\n\nprint(\"Evaluando modelo fine-tuned...\")\nresults_fine = evaluate_model(\"checkpoints/dpt_finetuned.pt\", eval_loader, DEVICE, NUM_CLASSES, pretrained=False)\nprint(\"Modelo Fine-Tuned:\", results_fine)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================\n# INTERACTIVO: ELECCIÓN DE MODELO + SUBIDA DE IMAGEN\n# ================================================\nfrom IPython.display import display\nfrom ipywidgets import Dropdown, FileUpload\nimport io\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as T\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# -------- Dropdown para elegir modelo ----------\nmodel_selector = Dropdown(\n    options=[('Fine-tuned (model)', 'model'), ('Preentrenado (model2)', 'model2')],\n    value='model',\n    description='Modelo:',\n)\ndisplay(model_selector)\n\n# -------- Subida de imagen ----------\nuploader = FileUpload(accept='image/*', multiple=False)\ndisplay(uploader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================\n# PROCESAMIENTO Y PREDICCIÓN - VERSIÓN ROBUSTA\n# ================================================\n\nif uploader.value:\n    # Detectar tipo de estructura\n    if isinstance(uploader.value, dict):\n        uploaded_file = list(uploader.value.values())[0]\n    elif isinstance(uploader.value, tuple):\n        uploaded_file = uploader.value[0]\n    else:\n        raise ValueError(\"Formato de archivo no reconocido\")\n\n    # Obtener imagen\n    image_bytes = uploaded_file['content']\n    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n    orig_image = image.copy()\n\n    # Transformación\n    transform = T.Compose([\n        T.Resize((512, 512)),\n        T.ToTensor(),\n        T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ])\n    input_tensor = transform(image).unsqueeze(0).to(DEVICE)\n\n    # Selección de modelo\n    selected_model = model if model_selector.value == 'model' else model2\n    selected_model.to(DEVICE)\n    selected_model.eval()\n\n    with torch.no_grad():\n        output = selected_model(input_tensor)\n        pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n\n    # Visualización\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.imshow(orig_image)\n    plt.title(\"Imagen Original\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(pred_mask, cmap=\"gray\")\n    plt.title(\"Máscara Predicha\")\n    plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nelse:\n    print(\"⬆️ Subí una imagen para visualizar la predicción.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Descongelar todo\nfor param in model.parameters():\n    param.requires_grad = True\n\n# O: solo descongelar encoder\nfor param in model.pretrained.parameters():\n    param.requires_grad = True\n\n# Nuevo optimizador con lr más bajo\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mv /kaggle/working/checkpoints/dpt_finetuned.pt /kaggle/working/output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -la /kaggle/working/output/dpt_finetuned.pt\n!date","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}