{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9319570,"sourceType":"datasetVersion","datasetId":5645157},{"sourceId":9483577,"sourceType":"datasetVersion","datasetId":5768881},{"sourceId":10386557,"sourceType":"datasetVersion","datasetId":6434565},{"sourceId":10805710,"sourceType":"datasetVersion","datasetId":6707189},{"sourceId":10923652,"sourceType":"datasetVersion","datasetId":6791481}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Frame generation**","metadata":{}},{"cell_type":"code","source":"#Libraries and functions\n\nimport cv2\nimport numpy as np\nimport glob # To find files that have an specific pattern using a wildcard\nimport os\n\n##############################VIDEO TO BE PROCESSED##############################\n#video_path          = '/kaggle/input/workzone/workzone2.mp4' #/kaggle/input/d/justinfb/workzone/workzone2.mp4\nvideo_path          = '/kaggle/input/d/justinfb/workzone/workzone2.mp4'\n#video_path          = '/kaggle/input/workzone6-1-25/CV_Work Zone/workzone.mp4'\nvideo_cv2           = cv2.VideoCapture(video_path)\nfps_cv2             = video_cv2.get(cv2.CAP_PROP_FPS)\n# TO-ASK: Why these values are hard-coded?\nvideo_initial_video = 3 * 60 + 48 # 3 minutes and 48 seconds.\nduration_video      = 45          # 45 seconds\n##############################VIDEO TO BE PROCESSED##############################\n\noutput_folder = '/kaggle/working/frames'\nif not os.path.exists(output_folder):\n    os.makedirs(output_folder)\n\ndef video_to_images(video_path, output_folder):\n    # Open the video file\n    video_capture = cv2.VideoCapture(video_path)\n    \n    # Check if the output folder exists, if not, create it\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n    \n    frame_count = 0\n    \n    # Read video frame by frame and save each frame\n    while True:\n        ret, frame = video_capture.read()\n        if not ret:\n            break\n        \n        # Create the filename with the frame number\n        filename = os.path.join(output_folder, f\"frame_{frame_count:04d}.png\")\n        \n        # Save the frame as an image\n        cv2.imwrite(filename, frame)\n        \n        frame_count += 1\n    \n    # Release the video capture object\n    video_capture.release()\n    \n    print(f\"Processed and saved {frame_count} frames to {output_folder}\")\n\n# Usage\nvideo_to_images(video_path, output_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:28:30.028104Z","iopub.execute_input":"2025-03-04T23:28:30.028336Z","iopub.status.idle":"2025-03-04T23:28:38.598318Z","shell.execute_reply.started":"2025-03-04T23:28:30.028310Z","shell.execute_reply":"2025-03-04T23:28:38.597455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Vision transformer**","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/isl-org/DPT.git\n\n# Download models and weights\n!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt\n!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\n!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-ade20k-53898607.pt\n!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-ade20k-b12dca68.pt\n    \n# Import weights\n!mv ./dpt_hybrid-ade20k-53898607.pt ./DPT/weights\n!mv ./dpt_large-ade20k-b12dca68.pt ./DPT/weights\n!mv ./dpt_large-midas-2f21e586.pt ./DPT/weights\n!mv ./dpt_hybrid-midas-501f0c75.pt ./DPT/weights\n\n# Pip install required libraries with last releases\n!pip install torch\n!pip install torchvision\n!pip install opencv-python\n!pip install timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:28:49.821214Z","iopub.execute_input":"2025-03-04T23:28:49.821557Z","iopub.status.idle":"2025-03-04T23:29:59.062101Z","shell.execute_reply.started":"2025-03-04T23:28:49.821527Z","shell.execute_reply":"2025-03-04T23:29:59.061031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil # To make actions in an easy way in the directories and files\nimport os\nimport cv2\n\ndef count_frames(cuts,video_path):\n    \"\"\"\n    Converts time-based video cuts (HH:MM:SS) into frame indices based on the video's FPS.\n    \n    Parameters:\n        cuts (list of tuples): List of (start_time, end_time) tuples in \"HH:MM:SS\" format.\n        video_path (str): Path to the video file.\n\n    Returns:\n        list: A list of start and end frame indices for each cut.\n    \"\"\"\n    # Empty list with frames\n    video  = cv2.VideoCapture(video_path)\n    fps    = video.get(cv2.CAP_PROP_FPS)\n    frames = []\n    \n    for idx, (start_time, duration) in enumerate(cuts):\n        # Start frame\n        start_seconds = int(cuts[idx][0].split(\":\")[2])\n        start_minutes = int(cuts[idx][0].split(\":\")[1])\n        start_hours   = int(cuts[idx][0].split(\":\")[0])\n        frame_start   = int((start_seconds + start_minutes * 60 + start_hours * 60 * 60) * fps)\n        frames.append(frame_start)\n\n        end_seconds = int(cuts[idx][1].split(\":\")[2])\n        end_minutes = int(cuts[idx][1].split(\":\")[1])\n        end_hours   = int(cuts[idx][1].split(\":\")[0])\n        frame_end   = int((end_seconds + end_minutes * 60 + end_hours * 60 * 60) * fps)\n        frames.append(frame_end)\n        \n    return frames\n\ndef count_all_frames_from_frames(dir_frame):\n    frame_list       = os.listdir(dir_frame)\n    folder_name      = dir_frame.split('/')[-1]\n    frames_range     = [0]\n    frames_range.append(len(frame_list))\n    return frames_range\n\ndef count_all_frames(dir_frame):\n    frame_list       = os.listdir(dir_frame)\n    folder_name      = dir_frame.split('/')[-1]\n    frames_range     = [0]\n    frames_range.append(len(frame_list))\n    return frames_range\n\ndef get_some_frames(dir_frame, offset, duration):\n    frames_range     = [offset]\n    frames_range.append(offset + duration)\n    return frames_range\n\ndef rename_files_for_processing(cuts,video_path,frame_dir):\n    # Empty list with frames\n    video  = cv2.VideoCapture(video_path)\n    fps    = video.get(cv2.CAP_PROP_FPS)\n    #frames = count_frames(cuts,video_path)\n    frames = count_all_frames_from_frames(frame_dir)\n        \n    #print(frames)\n    #print(fps)\n    #video_name = video_path.split('/')[-1]\n    #frame_name = frame_dir + video_name.split('.avi')[0]\n    frame_name = frame_dir + '/' + frame_dir.split('/')[-1]\n    # It goes through the required frames, it is divided by 2 because it comes in pairs\n    for frames_sample in range(int(len(frames)/2)):\n        print('Processing frame ' + frame_name + ' of frame pair ' + str(frames[0 + frames_sample * 2]) + ' to ' + str(frames[1 + frames_sample * 2]))\n        #It goes from start frame to the last of that pair\n        for copy_frames in range(frames[1  + frames_sample * 2] - frames[0 + frames_sample * 2]):\n            #To check names correctly choose\n            #print(frame_name + str(copy_frames + frames[0]) + '.jpg')\n            # To get the name of the new picture\n            file_name_from_frame = frame_name.split('/')[-1]\n            # The copy_frames + frames[0] gives the resulting frame to be processed\n            shutil.copyfile(frame_name + str(copy_frames + frames[0 + frames_sample * 2]) + '.jpg',\n                            '/kaggle/working/DPT/input/' + file_name_from_frame + str(copy_frames + frames[0 + frames_sample * 2]) + '.jpg')\n            \ndef rename_files_for_processing_custom_cuts(offset, duration,frame_dir):\n    # Empty list with frames\n    #video  = cv2.VideoCapture(video_path)\n    #fps    = video.get(cv2.CAP_PROP_FPS)\n    #frames = count_frames(cuts,video_path)\n    #frames = count_all_frames_from_frames(frame_dir)\n    frames = get_some_frames(frame_dir, offset, duration)\n    \n    #print(frames)\n    #print(fps)\n    #video_name = video_path.split('/')[-1]\n    #frame_name = frame_dir + video_name.split('.avi')[0]\n    #frame_name = frame_dir + '/' + frame_dir.split('/')[4]\n    #print(frame_dir.split('/')[4])\n    print('Processing frame from ' + str(frames[0]) + ' to ' + str(frames[1]))\n    for copy_frames in range(frames[1] - frames[0]):\n        #To check names correctly choose\n        #print(frame_name + str(copy_frames + frames[0]) + '.jpg')\n        # To get the name of the new picture\n        #file_name_from_frame = frame_name.split('/')[-1]\n        #print(file_name_from_frame)\n        #print(frame_name)\n        #print(str(copy_frames + frames[0]))\n        # The copy_frames + frames[0] gives the resulting frame to be processed\n        if ((copy_frames + frames[0]) < 10):\n            zeros_added = \"000\"\n        elif ((copy_frames + frames[0]) < 100):\n            zeros_added = \"00\"\n        elif ((copy_frames + frames[0]) < 1000):\n            zeros_added = \"0\"\n        else:\n            zeros_added = \"\"\n        shutil.copyfile(\"/kaggle/working/frames/frame_\" + zeros_added + str(copy_frames + frames[0]) + '.png',\n                        '/kaggle/working/DPT/input/' + \"frame_\" + zeros_added + str(copy_frames + frames[0]) + '.png')\n        \n        #shutil.copyfile(frame_name + str(copy_frames + frames[0]) + '.jpg',\n        #                '/kaggle/working/DPT/input/' + str(copy_frames + frames[0]) + '.jpg')\n            \ndef get_all_files(cuts,video_dir,frame_dir):\n    video_list = os.listdir(video_dir)\n    for video_path in video_list:\n        rename_files_for_processing(cuts,video_dir + video_path,frame_dir)\n        #rename_files_for_processing_custom_cuts(offset, duration,video_dir + video_path,frame_dir)\n        ################################################\n        break\n        \ndef get_all_files_custom(offset, duration,frame_dir):\n    #print(frame_dir)\n    rename_files_for_processing_custom_cuts(offset, duration, frame_dir)\n        \ndef initialize_system():\n    # Generate output directory\n    if(not(os.path.isdir('/kaggle/working/output'))):\n        os.mkdir('/kaggle/working/output')\n    \n    filename = \"/kaggle/working/DPT/run_monodepth.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"output_monodepth\"', '\"/kaggle/working/DPT/output_monodepth\"'))\n\n    filename = \"/kaggle/working/DPT/run_segmentation.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"output_semseg\"', '\"/kaggle/working/DPT/output_semseg\"'))\n    \n    filename = \"/kaggle/working/DPT/run_monodepth.py\"\n    text = open(filename).read()\n\n    #Here goes your files\n    open(filename, \"w+\").write(text.replace('\"input\"', '\"/kaggle/working/DPT/input/\"'))\n\n    filename = \"/kaggle/working/DPT/run_segmentation.py\"\n    text = open(filename).read()\n\n    #Here goes your files\n    open(filename, \"w+\").write(text.replace('\"input\"', '\"/kaggle/working/DPT/input/\"'))\n    \n    filename = \"/kaggle/working/DPT/run_monodepth.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"weights/', '\"/kaggle/working/DPT/weights/'))\n\n    filename = \"/kaggle/working/DPT/run_segmentation.py\"\n    text = open(filename).read()\n    open(filename, \"w+\").write(text.replace('\"weights/', '\"/kaggle/working/DPT/weights/'))\n    \ndef copy_files_to_output_depth(video_dir,cuts):\n    output_file_depth = '/kaggle/working/DPT/output_monodepth/'\n    output_file_segm  = '/kaggle/working/DPT/output_semseg/'\n    output_file       = '/kaggle/working/output/depth'\n    \n    video_list = os.listdir(video_dir)\n    \n    # Generate output directory\n    if(not(os.path.isdir(output_file))):\n        os.mkdir(output_file)\n\n    #Create zip files to extract them\n    # This for goes through all the video directories\n    for sample_picture_0 in video_list:\n        # We know that every video ends in 0.jpg, where 0 is the frame, so we must build each one\n        frame_name = sample_picture_0.split('.avi')[0]\n        frames     = count_frames(cuts,video_dir + sample_picture_0)\n        # It goes through the required frames, it is divided by 2 because it comes in pairs\n        for frames_sample in range(int(len(frames)/2)):\n            print('Processing frame ' + sample_picture_0 + ' of frame pair ' + str(frames_sample))\n            #It goes from start frame to the last of that pair\n            for copy_frames in range(frames[1 + frames_sample * 2] - frames[0 + frames_sample * 2]):\n                #To check names correctly choose\n                #print(frame_name + str(copy_frames + frames[0]) + '.jpg')\n\n                # The copy_frames + frames[0] gives the resulting frame to be processed for depth, png and pfm\n                shutil.move(output_file_depth + frame_name + str(copy_frames + frames[0 + frames_sample * 2]) + '.png',\n                            output_file + frame_name + str(copy_frames + frames[0 + frames_sample * 2]) + '.png')\n                shutil.move(output_file_depth + frame_name + str(copy_frames + frames[0 + frames_sample * 2]) + '.pfm',\n                            output_file + frame_name + str(copy_frames + frames[0 + frames_sample * 2]) + '.pfm')\n\ndef copy_files_to_output_segmentation(video_dir,cuts):\n    output_file_depth = '/kaggle/working/DPT/output_monodepth/'\n    output_file_segm  = '/kaggle/working/DPT/output_semseg/'\n    output_file       = '/kaggle/working/output/segmentation'\n    \n    video_list = os.listdir(video_dir)\n    \n    if(not(os.path.isdir(output_file))):\n        os.mkdir(output_file)\n\n    #Create zip files to extract them\n    # This for goes through all the video directories\n    for sample_picture_0 in video_list:\n        # We know that every video ends in 0.jpg, where 0 is the frame, so we must build each one\n        frame_name = sample_picture_0.split('.avi')[0]\n        frames     = count_frames(cuts,video_dir + sample_picture_0)\n        # It goes through the required frames, it is divided by 2 because it comes in pairs\n        for frames_sample in range(int(len(frames)/2)):\n            print('Processing frame ' + sample_picture_0 + ' of frame pair ' + str(frames_sample))\n            #It goes from start frame to the last of that pair\n            for copy_frames in range(frames[1 + frames_sample * 2] - frames[0 + frames_sample * 2]):\n                #To check names correctly choose\n                #print(frame_name + str(copy_frames + frames[0]) + '.jpg')\n\n                # The copy_frames + frames[0] gives the resulting frame to be processed for segmentation\n                shutil.move(output_file_segm + frame_name + str(copy_frames + frames[0 + frames_sample * 2]) + '.png',\n                            output_file + frame_name + str(copy_frames + frames[0 + frames_sample * 2]) + '.png')\n\ndef copy_output_monodepth_from_dir(frame_dir_video, offset, duration):\n    output_file_depth = '/kaggle/working/DPT/output_monodepth/'\n    #output_file_segm  = '/kaggle/working/DPT/output_semseg/'\n    input_file        = '/kaggle/working/DPT/input/'\n    frame_name        = frame_dir_video.split('/')[-1]\n    output_file       = '/kaggle/working/output/' + frame_name + '/monocular/'\n    #output_file       = '/kaggle/working/output/segmentation'\n    \n    list_files = os.listdir(output_file_depth)\n    frames     = count_all_frames_from_frames(frame_dir_video)\n    \n    #if(not(os.path.isdir(output_file))):\n    #    os.mkdir(output_file)\n    \n    os.makedirs(output_file, exist_ok=True)\n    \n    for studied_frame in list_files:\n        if studied_frame.endswith('.png'):\n            shutil.move(output_file_depth + studied_frame,output_file + studied_frame)\n            #/kaggle/working/DPT/input/soads_2024-06-26-16-40-11-_front_camera_image_compressed378.jpg\n            # Remove already process frame\n            os.remove(input_file + studied_frame.replace('.png','.png'))\n        else:\n            os.remove(output_file_depth + studied_frame)\n        \ndef copy_outputs_segmentation_from_dir(frame_dir_video, offset, duration):\n    #output_file_depth = '/kaggle/working/DPT/output_monodepth/'\n    output_file_segm  = '/kaggle/working/DPT/output_semseg/'\n    input_file        = '/kaggle/working/DPT/input/'\n    frame_name        = frame_dir_video.split('/')[-1]\n    output_file       = '/kaggle/working/output/' + frame_name + '/segmentation/'\n    #output_file       = '/kaggle/working/output/segmentation'\n    \n    list_files = os.listdir(output_file_segm)\n    #frames     = get_some_frames(dir_frame, offset, duration)\n    frames     = count_all_frames_from_frames(frame_dir_video)\n    \n    #if(not(os.path.isdir(output_file))):\n    #    os.mkdir(output_file)\n    \n    os.makedirs(output_file, exist_ok=True)\n    \n    for studied_frame in list_files:\n        if studied_frame.endswith('.png'):\n            shutil.move(output_file_segm + studied_frame, output_file + studied_frame)\n            os.remove(input_file + studied_frame.replace('.png','.png'))\n        else:\n            os.remove(output_file_segm + studied_frame)\n        \ndef execute_monodepth(video_dir,cuts):\n    !python /kaggle/working/DPT/run_monodepth.py\n    copy_files_to_output_depth(video_dir,cuts)\n    \ndef execute_monodepth_dir(video_dir, offset, duration):\n    !python /kaggle/working/DPT/run_monodepth.py\n    copy_output_monodepth_from_dir(video_dir, offset, duration)\n\ndef execute_segmentation(video_dir,cuts):\n    !python /kaggle/working/DPT/run_segmentation.py\n    copy_files_to_output_segmentation(video_dir,cuts)\n    \ndef execute_segmentation_dir(video_dir, offset, duration):\n    !python /kaggle/working/DPT/run_segmentation.py\n    copy_outputs_segmentation_from_dir(video_dir, offset, duration) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:30:02.445010Z","iopub.execute_input":"2025-03-04T23:30:02.445377Z","iopub.status.idle":"2025-03-04T23:30:02.591048Z","shell.execute_reply.started":"2025-03-04T23:30:02.445345Z","shell.execute_reply":"2025-03-04T23:30:02.590086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"frame_dirs      = output_folder\nlist_dir_frames = os.listdir(frame_dirs)\n\n#print(list_dir_frames)\n\n#max_frames      = count_all_frames_from_frames(frame_dirs + list_dir_frames[video_num])[1]\n\nmax_frames      = count_all_frames(frame_dirs)[1]\n#max_frames      = (video_initial_video + duration_video) * fps_cv2\n#offset          = video_initial_video * fps_cv2\n#print(max_frames)\n#break\n\n\nframe_counter   = max_frames\n#frame_counter   = 5200\n#frame_counter   = 1200\nprint('Monodepth: Maximum frames are: ' + str(max_frames))\nframe_segm      = 100\noffset          = 0\ninitial_offset  = offset\n\nwhile True:\n    get_all_files_custom(offset, frame_segm, frame_dirs)\n    initialize_system()\n    execute_monodepth_dir(frame_dirs, offset, frame_segm)\n    offset        += frame_segm\n    #frame_counter -= frame_segm\n    if(offset + frame_segm >= max_frames) or (offset >= frame_counter):\n        break\n\n# If there are some frames left\nif(offset + frame_segm > max_frames):\n    frame_counter_diff = max_frames - offset\n    print(frame_counter_diff)\n    get_all_files_custom(offset, frame_counter_diff, frame_dirs)\n    initialize_system()\n    #Monodepth\n    execute_monodepth_dir(frame_dirs, offset, frame_counter_diff)\n    \nprint('Segmentation: Maximum frames are: ' + str(max_frames))\nframe_segm      = 100\noffset          = 0\ninitial_offset  = offset\n\nwhile True:\n    get_all_files_custom(offset, frame_segm, frame_dirs)\n    initialize_system()\n    #Segmentation\n    execute_segmentation_dir(frame_dirs, offset, frame_segm)\n    offset        += frame_segm\n    #frame_counter -= frame_segm\n    if(offset + frame_segm >= max_frames) or (offset >= frame_counter):\n        break\n\n# If there are some frames left\nif(offset + frame_segm > max_frames):\n    frame_counter_diff = max_frames - offset\n    print(frame_counter_diff)\n    get_all_files_custom(offset, frame_counter_diff, frame_dirs)\n    initialize_system()\n    #Segmentation\n    execute_segmentation_dir(frame_dirs, offset, frame_counter_diff)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:30:09.701522Z","iopub.execute_input":"2025-03-04T23:30:09.702300Z","iopub.status.idle":"2025-03-04T23:33:14.857752Z","shell.execute_reply.started":"2025-03-04T23:30:09.702263Z","shell.execute_reply":"2025-03-04T23:33:14.856863Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Map generation**","metadata":{}},{"cell_type":"code","source":"!pip install open3d\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport imageio.v3 as iio\nimport matplotlib.pyplot as plt\nimport open3d as o3d\nimport os\nimport math\nimport cv2\nimport copy\nfrom PIL import Image\nfrom numpy import asarray","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:33:38.758773Z","iopub.execute_input":"2025-03-04T23:33:38.759165Z","iopub.status.idle":"2025-03-04T23:34:06.114978Z","shell.execute_reply.started":"2025-03-04T23:33:38.759129Z","shell.execute_reply":"2025-03-04T23:34:06.114219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def increase_brightness(image_path, value):\n    # Read the image\n    image = cv2.imread(image_path)\n    # Check if image is loaded successfully\n    if image is None:\n        raise ValueError(f\"Image at path '{image_path}' could not be loaded.\")\n    # Convert the image to HSV (Hue, Saturation, Value) color space\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    # Split the image into three channels: H, S, and V\n    h, s, v = cv2.split(hsv)\n    # Increase the V (value/brightness) channel by the given value\n    v = cv2.add(v, value)\n    # Merge the channels back\n    final_hsv = cv2.merge((h, s, v))\n    # Convert the HSV image back to BGR color space\n    brightened_image = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n    return brightened_image\n\n#This is for Vision Transformers because it gives an inverse relation of white and black is.\ndef normalize_image_to_pcd(input_path,brightness_value):\n    brightness_image  = increase_brightness(input_path,brightness_value)\n    cv2.imwrite('brightened_image.jpg', brightness_image)\n    input_image       = Image.open('brightened_image.jpg').convert('L')\n    input_image_array = input_image.resize((640,480))\n    # It reverse depth making black, white and viceversa.\n    #max_pixel_value   = input_image_array\n    reverse_depth     = 255 - asarray(input_image_array)\n    #print(len(reverse_depth))\n    return(reverse_depth)\n\n#It extracts color from image.\ndef extract_color_image(img_path):\n    img = Image.open(img_path)\n    if img.mode != 'RGB':\n        img = img.convert('RGB')\n    if img.size != (640,480):\n        img = img.resize((640, 480))\n    img = asarray(img)\n    img = img/255.0 # normalize RGB values to [0, 1]\n    height, width, channels = img.shape\n    length = height * width\n\n    color_list = img.reshape((length, channels)) # array of RGB values\n    return color_list\n\n#It creates the PCD from depth.\ndef depth_to_pcd(depth_path, color_image_path, brightness_value):\n    # Depth camera parameters from NYU:\n    FX_DEPTH = 5.8262448167737955e+02\n    FY_DEPTH = 5.8269103270988637e+02\n    CX_DEPTH = 3.1304475870804731e+02\n    CY_DEPTH = 2.3844389626620386e+02\n    \n    # It only has one channel it is in grey scale, it change size and invert greyscale\n    depth_image = normalize_image_to_pcd(depth_path,brightness_value)\n    \n    # get depth resolution:\n    height, width = depth_image.shape\n    length = height * width\n    # compute indices:\n    jj = np.tile(range(width), height)\n    ii = np.repeat(range(height), width)\n    # rechape depth image\n    z = depth_image.reshape(length)\n    # compute pcd:\n    pcd = np.dstack([(ii - CX_DEPTH) * z / FX_DEPTH,\n                 (jj - CY_DEPTH) * z / FY_DEPTH,\n                 z]).reshape((length, 3))\n\n    pcd_o3d = o3d.geometry.PointCloud()\n    pcd_o3d.points = o3d.utility.Vector3dVector(pcd)\n    colors = extract_color_image(color_image_path)\n    pcd_o3d.colors = o3d.utility.Vector3dVector(colors)\n    # Define the box parameters\n    center         = np.array([0.0, 0.0, 225])\n    # Consider 70\n    extent         = np.array([400, 400, 100])\n    rotation       = np.eye(3)  # Identity matrix for no rotation\n    # Cut the points inside the box, eliminate point in infinite or far far away\n    pcd_o3d        = cut_points_in_box(pcd_o3d, center, extent, rotation)\n    #o3d.io.write_point_cloud(output_pcd_path, pcd_o3d)\n    return pcd_o3d\n\ndef cut_points_in_box(point_cloud, box_center, box_extent, box_rotation=None):\n    \"\"\"\n    Cut points inside a box from the point cloud.\n    \n    Parameters:\n    - point_cloud: open3d.geometry.PointCloud, the input point cloud.\n    - box_center: list or np.array of shape (3,), the center of the box.\n    - box_extent: list or np.array of shape (3,), the size of the box (x, y, z dimensions).\n    - box_rotation: optional, np.array of shape (3,3), the rotation matrix of the box.\n    \n    Returns:\n    - point_cloud_outside_box: open3d.geometry.PointCloud, the point cloud with points inside the box removed.\n    \"\"\"\n    # Create an AxisAlignedBoundingBox or OrientedBoundingBox\n    if box_rotation is None:\n        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=box_center - 0.5 * box_extent,\n                                                   max_bound=box_center + 0.5 * box_extent)\n    else:\n        bbox = o3d.geometry.OrientedBoundingBox(center=box_center,\n                                                R=box_rotation,\n                                                extent=box_extent)\n    \n    # Get the indices of points inside the box\n    indices_inside_box = bbox.get_point_indices_within_bounding_box(point_cloud.points)\n    \n    # Select points outside the box\n    point_cloud_outside_box = point_cloud.select_by_index(indices_inside_box, invert=True)\n    \n    return point_cloud_outside_box\n\ndef gps_to_decimeters(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Convert two GPS coordinates to meters relative to the first coordinate.\n    \n    :param lat1: Latitude of the first coordinate in decimal degrees\n    :param lon1: Longitude of the first coordinate in decimal degrees\n    :param lat2: Latitude of the second coordinate in decimal degrees\n    :param lon2: Longitude of the second coordinate in decimal degrees\n    :return: A tuple (x, y) representing the second coordinate in meters relative to the first\n    \"\"\"\n    # Radius of the Earth in meters\n    R = 6378137.0\n    \n    # Convert degrees to radians\n    lat1_rad = math.radians(lat1)\n    lon1_rad = math.radians(lon1)\n    lat2_rad = math.radians(lat2)\n    lon2_rad = math.radians(lon2)\n    \n    # Differences in coordinates\n    dlat = lat2_rad - lat1_rad\n    dlon = lon2_rad - lon1_rad\n    \n    # Equirectangular approximation\n    x_meters = dlon * math.cos((lat1_rad + lat2_rad) / 2.0) * R\n    y_meters = dlat * R\n    \n    # Convert to centimeters\n    x_decimeters = x_meters * 10\n    y_decimeters = y_meters * 10\n    \n    return x_decimeters, y_decimeters\n\ndef mean_filter_column(matrix, window_size):\n    \"\"\"\n    Apply a mean filter to each column of a matrix individually.\n    \n    Parameters:\n    matrix (np.ndarray): Input matrix of shape (n, m)\n    window_size (int): Size of the moving window for the mean filter\n    \n    Returns:\n    np.ndarray: Matrix with mean filter applied to each column\n    \"\"\"\n    if window_size < 1:\n        raise ValueError(\"window_size must be at least 1\")\n        \n    # Create a new matrix to store the results\n    filtered_matrix = np.zeros_like(matrix)\n    \n    # Get the number of rows and columns\n    n, m = matrix.shape\n    \n    # Apply the mean filter to each column\n    for col in range(m):\n        column = matrix[:, col]\n        filtered_column = np.zeros(n)\n        \n        # Apply the mean filter to the current column\n        for row in range(n):\n            start_idx = max(0, row - window_size // 2)\n            end_idx = min(n, row + window_size // 2 + 1)\n            filtered_column[row] = np.mean(column[start_idx:end_idx])\n        \n        filtered_matrix[:, col] = filtered_column\n    \n    return filtered_matrix\n\ndef vector_to_polar(x, y):\n    # Compute the magnitude\n    magnitude = math.sqrt(x**2 + y**2)\n    \n    # Compute the angle in radians\n    theta = math.atan2(y, x)  # atan2 returns the angle in radians\n\n    return magnitude, theta\n\ndef normalize_angle(angle):\n    \"\"\"\n    Normalize an angle to the range [-pi, pi].\n\n    Parameters:\n    angle (float): The angle in radians to normalize.\n\n    Returns:\n    float: The normalized angle in the range [-pi, pi].\n    \"\"\"\n    # Normalize the angle to the range [-pi, pi]\n    normalized_angle = (angle + math.pi) % (2 * math.pi) - math.pi\n    return normalized_angle\n\n# This function gives a coordinate system with relative movements and accumulated\ndef coordinated_system(gps_path):\n    # It eliminates first element to make the relative movement\n    file_gps_locations = open(gps_path, \"r\")\n    # It reads the line twice to get rid of the title.\n    prior_element      = file_gps_locations.readline()\n    prior_element      = file_gps_locations.readline()\n    relative_magnitude = []\n    relative_theta     = []\n    # This keeps moving the origin and orientation\n    absolute_x_matrix  = []\n    absolute_x_scalar  = 0\n    absolute_y_matrix  = []\n    absolute_y_scalar  = 0\n    absolute_theta     = []\n    accumulated_theta  = 0\n    timestamp_matrix   = []\n    delta_timestamp    = []\n    for line_gps in file_gps_locations:\n        timestamp_0        = int(float(prior_element.split(',')[0]))\n        latitude_0         = float(prior_element.split(',')[2])\n        longitude_0        = float(prior_element.split(',')[3])\n        timestamp_1        = int(float(line_gps.split(',')[0]))\n        latitude_1         = float(line_gps.split(',')[2])\n        longitude_1        = float(line_gps.split(',')[3])\n        # Update element\n        prior_element      = line_gps\n        x, y               = gps_to_decimeters(latitude_0, longitude_0, latitude_1, longitude_1)\n        magnitude, theta   = vector_to_polar(x, y)\n        # Accumulates relative trajectory.\n        relative_magnitude.append(magnitude)\n        # The sampling rate is approximately 30 Hz, then if the truck moves not even 1 cm in that period, then, it is still\n        if absolute_x_scalar < 0.1 and absolute_y_scalar < 0.1:\n            theta = 0\n        relative_theta.append(theta)\n        # It translates origin and accumulates theta\n        absolute_x_scalar  += x\n        absolute_y_scalar  += y\n        accumulated_theta  += theta\n        absolute_x_matrix.append(absolute_x_scalar)\n        absolute_y_matrix.append(absolute_y_scalar)\n        absolute_theta.append(accumulated_theta)\n        timestamp_matrix.append(timestamp_1)\n        delta_timestamp.append(timestamp_1 - timestamp_0)\n    # It returns matrix with accumulated values\n    result_matrix = relative_magnitude\n    result_matrix = np.vstack((result_matrix,relative_theta))\n    result_matrix = np.vstack((result_matrix,absolute_x_matrix))\n    result_matrix = np.vstack((result_matrix,absolute_y_matrix))\n    result_matrix = np.vstack((result_matrix,absolute_theta))\n    result_matrix = np.vstack((result_matrix,timestamp_matrix))\n    result_matrix = np.transpose(result_matrix)\n    mean_time     = np.mean(delta_timestamp)\n    return result_matrix,mean_time\n\ndef fps_sinchronize(coordinate_matrix, video_path_timestamps, fps_video, video_duration_seconds):\n    file_video_l = open(video_path_timestamps,\"r\")\n    first_time_v = int(file_video_l.readline())\n    # It is used to calculate the required timestamps\n    frame_cal_ar = []\n    for frame_number in range(video_duration_seconds * fps_video):\n        frame_cal_ar.append(first_time_v + frame_number * 1e9 / fps_video)\n    magnitude_m  = []\n    rel_theta_m  = []\n    absolute_x_m = []\n    absolute_y_m = []\n    absolute_t_m = []\n    timestamp_m  = []\n    \n    log_file_path = 'log_timestamps_pcd.txt'\n    if not os.path.exists(log_file_path):\n        os.mknod(log_file_path)\n        \n    file = open(log_file_path,'a')\n    # Look for the frame with the lower difference\n    for frame_cal in frame_cal_ar:\n        # To initialize value\n        threshold      = frame_cal\n        magnitude      = 0\n        relative_theta = 0\n        absolute_x     = 0\n        absolute_y     = 0\n        absolute_theta = 0\n        timestamp      = 0\n        # Look through all the rows\n        for index in range(coordinate_matrix.shape[0]):\n            cal_time_stamp = abs(frame_cal - coordinate_matrix[index,5])\n            if(cal_time_stamp < threshold):\n                magnitude  = coordinate_matrix[index,0]\n                r_theta    = coordinate_matrix[index,1]\n                absolute_x = coordinate_matrix[index,2]\n                absolute_y = coordinate_matrix[index,3]\n                absolute_t = coordinate_matrix[index,4]\n                timestamp  = coordinate_matrix[index,5]\n                threshold  = cal_time_stamp\n        # Save the last value with shortest threshold.\n        magnitude_m.append(magnitude)\n        rel_theta_m.append(r_theta)\n        absolute_x_m.append(absolute_x)\n        absolute_y_m.append(absolute_y)\n        absolute_t_m.append(absolute_t)\n        timestamp_m.append(timestamp)\n        file.write(str(magnitude) + '/' + str(r_theta) + '/' + str(absolute_x) + '/' + str(absolute_y) + '/' + str(timestamp) + '\\n')\n        \n    result_matrix = magnitude_m\n    result_matrix = np.vstack((result_matrix,rel_theta_m))\n    result_matrix = np.vstack((result_matrix,absolute_x_m))\n    result_matrix = np.vstack((result_matrix,absolute_y_m))\n    result_matrix = np.vstack((result_matrix,absolute_t_m))\n    result_matrix = np.vstack((result_matrix,timestamp_m))\n    result_matrix = np.transpose(result_matrix)\n    file.close()\n    return result_matrix\n\n\n\n\ndef fps_sinchronize_frames(coordinate_matrix, video_path_timestamps, fps_video, video_duration_frames):\n    file_video_l = open(video_path_timestamps,\"r\")\n    file_vid_rl  = file_video_l.readline()\n    if(not(file_vid_rl.isnumeric())):\n        file_vid_rl = file_vid_rl.split('.')[0]\n    first_time_v = int(file_vid_rl)\n    # It is used to calculate the required timestamps\n    frame_cal_ar = []\n    for frame_number in range(video_duration_frames):\n        frame_cal_ar.append((first_time_v + frame_number / fps_video) * 1e9)\n    magnitude_m  = []\n    rel_theta_m  = []\n    absolute_x_m = []\n    absolute_y_m = []\n    absolute_t_m = []\n    timestamp_m  = []\n    \n    log_file_path = 'log_timestamps_pcd.txt'\n    if not os.path.exists(log_file_path):\n        os.mknod(log_file_path)\n        \n    file = open(log_file_path,'a')\n    # Look for the frame with the lower difference\n    for frame_cal in frame_cal_ar:\n        # To initialize value\n        threshold      = frame_cal\n        magnitude      = 0\n        r_theta        = 0\n        absolute_x     = 0\n        absolute_y     = 0\n        absolute_t     = 0\n        absolute_theta = 0\n        timestamp      = 0\n        # Look through all the rows\n        for index in range(coordinate_matrix.shape[0]):\n            cal_time_stamp = abs(frame_cal - coordinate_matrix[index,5])\n            if(cal_time_stamp < threshold):\n                magnitude  = coordinate_matrix[index,0]\n                r_theta    = coordinate_matrix[index,1]\n                absolute_x = coordinate_matrix[index,2]\n                absolute_y = coordinate_matrix[index,3]\n                absolute_t = coordinate_matrix[index,4]\n                timestamp  = coordinate_matrix[index,5]\n                threshold  = cal_time_stamp\n        # Save the last value with shortest threshold.\n        magnitude_m.append(magnitude)\n        rel_theta_m.append(r_theta)\n        absolute_x_m.append(absolute_x)\n        absolute_y_m.append(absolute_y)\n        absolute_t_m.append(absolute_t)\n        timestamp_m.append(timestamp)\n        file.write(str(magnitude) + '/' + str(r_theta) + '/' + str(absolute_x) + '/' + str(absolute_y) + '/' + str(timestamp) + '\\n')\n        \n    result_matrix = magnitude_m\n    result_matrix = np.vstack((result_matrix,rel_theta_m))\n    result_matrix = np.vstack((result_matrix,absolute_x_m))\n    result_matrix = np.vstack((result_matrix,absolute_y_m))\n    result_matrix = np.vstack((result_matrix,absolute_t_m))\n    result_matrix = np.vstack((result_matrix,timestamp_m))\n    result_matrix = np.transpose(result_matrix)\n    file.close()\n    return result_matrix\n\ndef load_point_clouds_rotated_translated(pcd_front_rear, theta, origin_x, origin_y, voxel_size=0.0):\n    #Rotations are relative to front or forward\n    rotate_rear            = (np.pi, 0, 0)\n    rotate_theta           = (theta, 0, 0)\n    # To change magnitude from centimeters to decimeters\n    translate_mt           = (0, origin_x, origin_y)\n    \n    # Calculate first front\n    mesh                   = copy.deepcopy(pcd_front_rear[0])\n    rotated_theta          = np.eye(4)\n    rotated_theta[:3, :3]  = pcd_front_rear[0].get_rotation_matrix_from_xyz(rotate_theta)\n    # Move pcd forward a magnitude and then rotated theta radians. In this order to remain forward\n    mesh                   = mesh.translate(translate_mt)\n    mesh                   = mesh.transform(rotated_theta)\n    pcd_down_front         = mesh.voxel_down_sample(voxel_size=voxel_size)\n    \n    # Calculate then rear\n    rotated_matrix         = np.eye(4)\n    rotated_matrix[:3, :3] = pcd_front_rear[1].get_rotation_matrix_from_xyz(rotate_rear)\n    mesh                   = copy.deepcopy(pcd_front_rear[1]).transform(rotated_matrix)\n    rotated_theta          = np.eye(4)\n    rotated_theta[:3, :3]  = pcd_front_rear[1].get_rotation_matrix_from_xyz(rotate_theta)\n    # Rotate pcd forward and position it x and y decemiters. In this order to remain forward\n    mesh                   = mesh.transform(rotated_theta)\n    mesh                   = mesh.translate(translate_mt)\n    pcd_down_rear          = mesh.voxel_down_sample(voxel_size=voxel_size)\n    \n    # Define the box parameters\n    #center                 = np.array([0.0, 0.0, 225])\n    # Consider 70\n    #extent                 = np.array([400, 400, 100])\n    #rotation               = np.eye(3)  # Identity matrix for no rotation\n    # Cut the points inside the box, eliminate point in infinite or far far away\n    #pcd_front_outside_box  = cut_points_in_box(pcd_down_front, center, extent, rotation)\n    \n    #return pcd_down_front + pcd_down_rear\n    # For now just collect front, rear has a pipe in the middle that will get noise into the scene\n    return pcd_down_front\n\ndef accumulate_pcd(pcds):\n    pcd_join_pcd = o3d.geometry.PointCloud()\n    for current_pcd in pcds:\n        pcd_join_pcd += current_pcd\n    # Returns PCD recollection \n    return pcd_join_pcd\n\ndef map_generation(forward_depth_path, forward_color_path, chunks_reduction, video_duration, fps, forward_file_name_begin, rear_file_name_begin, coordinated_matrix):\n    brightness_value   = 0\n    voxel_size         = 2\n    pcd_chunks_accumul = []\n\n    for part_of_map in range(chunks_reduction):\n        # To do it with a blank each time\n        pcd_accumulated = []\n        chunk_duration  = int(video_duration * fps / chunks_reduction)\n        offset          = part_of_map * chunk_duration\n        for index in range(chunk_duration):\n            print('Processing frame: ' +  str(offset + index))\n            pcd_studied       = []\n            forward_file_name = forward_file_name_begin + str(offset + index)\n            rear_file_name    = rear_file_name_begin + str(offset + index)\n            depth_file_name_f = forward_depth_path + forward_file_name + '.png'\n            color_file_name_f = forward_color_path + forward_file_name + '.jpg'\n            depth_file_name_r = forward_depth_path + forward_file_name + '.png'\n            color_file_name_r = forward_color_path + forward_file_name + '.jpg'\n            # Accumulate front and then rear for the next function\n            theta             = coordinated_matrix[offset + index,1]\n            origin_x          = coordinated_matrix[offset + index,2]\n            origin_y          = coordinated_matrix[offset + index,3]\n            pcd_studied.append(depth_to_pcd(depth_file_name_f, color_file_name_f, brightness_value))\n            pcd_studied.append(depth_to_pcd(depth_file_name_r, color_file_name_r, brightness_value))\n            pcd_accumulated.append(load_point_clouds_rotated_translated(pcd_studied, theta, origin_x, origin_y, voxel_size))\n        pcd_chunks_accumul.append(pcd_accumulated)\n    return pcd_chunks_accumul\n\ndef map_generation_frames(forward_depth_path, forward_color_path, chunks_reduction, video_duration_frames, fps, forward_file_name_begin, coordinated_matrix):\n    brightness_value   = 0\n    voxel_size         = 2\n    pcd_chunks_accumul = []\n    \n    num_digits = len(str(video_duration_frames))\n\n    for part_of_map in range(chunks_reduction):\n        # To do it with a blank each time\n        pcd_accumulated = []\n        chunk_duration  = int(video_duration_frames / chunks_reduction)\n        offset          = part_of_map * chunk_duration\n        for index in range(chunk_duration):\n            print('Processing frame: ' +  str(offset + index))\n            pcd_studied       = []\n            #/kaggle/working/depth/soads_2024-06-26-16-51-42-_front_camera_image_compressed_1309.jpg\n            #/kaggle/working/color/soads_2024-06-26-16-51-42-_front_camera_image_compressed325.jpg\n            \n            formatted_number = f\"{(offset + index):0{num_digits}d}\"\n\n            print(f\"offset: {offset}, index: {index}, num_digits: {num_digits}, formatted_number: {formatted_number}\")\n            \n            # For 1 and 4\n            #forward_file_name_d = forward_file_name_begin + str(offset + index)\n            forward_file_name_d = forward_file_name_begin + formatted_number\n            # For 2 and 3\n            #forward_file_name_d = forward_file_name_begin + '_complete' + str(offset + index)\n            \n            forward_file_name_c = forward_file_name_d\n            depth_file_name_f = forward_depth_path + forward_file_name_d + '.png'\n            color_file_name_f = forward_color_path + forward_file_name_c + '.png'\n            # Accumulate front and then rear for the next function\n            theta             = coordinated_matrix[offset + index,1]\n            origin_x          = coordinated_matrix[offset + index,2]\n            origin_y          = coordinated_matrix[offset + index,3]\n            pcd_studied.append(depth_to_pcd(depth_file_name_f, color_file_name_f, brightness_value))\n            pcd_studied.append(depth_to_pcd(depth_file_name_f, color_file_name_f, brightness_value))\n            pcd_accumulated.append(load_point_clouds_rotated_translated(pcd_studied, theta, origin_x, origin_y, voxel_size))\n        pcd_chunks_accumul.append(pcd_accumulated)\n    return pcd_chunks_accumul\n\ndef frame_extractor(input_video, output_dir):\n    video_local = cv2.VideoCapture(input_video)\n    os.makedirs(output_dir, exist_ok=True)\n    i = 0\n    while(video_local.isOpened()):\n        ret, frame = video_local.read()\n        if ret == False:\n            break\n        cv2.imwrite(output_dir + input_video.split(\"/\")[-1].split(\".\")[0] + str(i) + '.jpg',frame)\n        i+=1\n    video_local.release()\n    return i\n\ndef synchronize_video_duration(coordinated_matrix,delta_timestamp_gps,fps_video,video_initial_seconds,duration_seconds):\n    rounded_fps_ms_gps  = round(1000 / delta_timestamp_gps)\n    rounded_fps_ms_vid  = round(fps_video)\n    ratio_gps_vid       = math.ceil(rounded_fps_ms_gps / rounded_fps_ms_vid)\n    ratio_vid_gps       = math.ceil(rounded_fps_ms_vid / rounded_fps_ms_gps)\n    video_coordinates   = []\n    \n    video_initial_ms    = video_initial_seconds * 1000\n    duration_seconds_ms = duration_seconds * 1000\n    # We require an int number\n    initial_frame_video = round(video_initial_ms / rounded_fps_ms_vid)\n    \n    final_frame_video   = initial_frame_video + round(duration_seconds_ms / rounded_fps_ms_vid)\n    \n    video_coordinates = match_gps_to_video(coordinated_gps_matrix, fps_video, fps_gps)\n    \n    if (rounded_fps_ms_gps < rounded_fps_ms_vid):\n        for frame_studied in range(final_frame_video - initial_frame_video):\n            for frame_studied_gps in range(ratio_gps_vid):\n                #print(coordinated_matrix[(frame_studied + initial_frame_video) * ratio_gps_vid])\n                video_coordinates.append(coordinated_matrix[(frame_studied + initial_frame_video) ])\n    else:\n        for frame_studied in range(final_frame_video - initial_frame_video):\n            #print(coordinated_matrix[(frame_studied + initial_frame_video) * ratio_gps_vid])\n            video_coordinates.append(coordinated_matrix[(frame_studied + initial_frame_video) * ratio_gps_vid])\n    return video_coordinates\n\ndef match_gps_to_video(coordinated_gps_matrix, fps_video, fps_gps):\n    \"\"\"\n    Adjusts GPS data to match the frame rate of the video.\n    If fps_video > fps_gps, it interpolates between points.\n    If fps_gps > fps_video, it erodes the GPS matrix by downsampling.\n\n    Parameters:\n    coordinated_gps_matrix (ndarray): The GPS data as a 2D array of shape (x, 6).\n    fps_video (int): Frame rate of the video.\n    fps_gps (int): Frame rate of the GPS data.\n\n    Returns:\n    ndarray: Adjusted GPS data matching the video frame rate.\n    \"\"\"\n\n    if fps_video > fps_gps:\n        # Case 1: Interpolation (fps_video > fps_gps)\n        ratio = fps_video / fps_gps\n        interpolated_gps_data = []\n\n        for i in range(len(coordinated_gps_matrix) - 1):\n            # Add the original GPS point\n            interpolated_gps_data.append(coordinated_gps_matrix[i])\n\n            # Calculate the number of interpolated points between current and next GPS point\n            num_interpolated_points = int(ratio) - 1\n\n            # Linearly interpolate between the current and next GPS point\n            for j in range(1, num_interpolated_points + 1):\n                interpolated_value = coordinated_gps_matrix[i] + (j / (num_interpolated_points + 1)) * (\n                    coordinated_gps_matrix[i + 1] - coordinated_gps_matrix[i]\n                )\n                interpolated_gps_data.append(interpolated_value)\n\n        # Add the last GPS point\n        interpolated_gps_data.append(coordinated_gps_matrix[-1])\n\n        return np.array(interpolated_gps_data)\n\n    else:\n        # Case 2: Erosion (fps_gps > fps_video)\n        ratio = fps_gps / fps_video\n        eroded_gps_data = []\n\n        # Downsample the GPS data by selecting every 'ratio'-th point\n        for i in range(0, len(coordinated_gps_matrix), int(ratio)):\n            eroded_gps_data.append(coordinated_gps_matrix[i])\n\n        # Ensure the last GPS point is included\n        if not np.array_equal(eroded_gps_data[-1], coordinated_gps_matrix[-1]):\n            eroded_gps_data.append(coordinated_gps_matrix[-1])\n\n        return np.array(eroded_gps_data)\n\ndef truncate_gps_data(coordinated_gps_matrix, fps_video, fps_gps, video_initial_seconds, duration_seconds):\n    \"\"\"\n    Truncates the GPS data to match a video segment starting from\n    video_initial_seconds and lasting duration_seconds.\n\n    Parameters:\n    coordinated_gps_matrix (ndarray): The GPS data as a 2D array of shape (x, 6).\n    fps_video (int): Frame rate of the video.\n    fps_gps (int): Frame rate of the GPS data.\n    video_initial_seconds (int): Start time in seconds for truncation.\n    duration_seconds (int): Duration in seconds for the segment.\n\n    Returns:\n    ndarray: Truncated and adjusted GPS data for the video segment.\n    \"\"\"\n    # Adjust GPS data to match the video frame rate\n    adjusted_gps_data = match_gps_to_video(coordinated_gps_matrix, fps_video, fps_gps)\n\n    # Calculate the total number of video frames\n    total_video_frames = len(adjusted_gps_data)\n\n    # Calculate the starting and ending frame indices\n    start_frame = int(video_initial_seconds * fps_video)\n    end_frame = start_frame + int(duration_seconds * fps_video)\n\n    # Truncate the adjusted GPS data to the specified segment\n    truncated_gps_data = adjusted_gps_data[start_frame:end_frame]\n\n    return truncated_gps_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:42:18.560785Z","iopub.execute_input":"2025-03-04T23:42:18.561198Z","iopub.status.idle":"2025-03-04T23:42:18.616166Z","shell.execute_reply.started":"2025-03-04T23:42:18.561167Z","shell.execute_reply":"2025-03-04T23:42:18.615136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GPS coordinates where video started\nvideo_initial = 3 * 60 + 48 # 3 minutes and 48 seconds.\nduration      = 45          # 45 seconds\n#gps_path      = \"/kaggle/input/vtti09-04-videos-from-the-road/CV_Work Zone/GX019270_HERO11 Black-GPS9.csv\"\ngps_path      = \"/kaggle/input/gx019270-hero11-black-gps9-csv/GX019270_HERO11 Black-GPS9.csv\"\n\nvideo  = cv2.VideoCapture(video_path)\nfps    = video.get(cv2.CAP_PROP_FPS)\nwindow_filter_size    = 160\n\ncoordinated_matrix,timestamp = coordinated_system(gps_path)\ncoordinated_matrix           = mean_filter_column(coordinated_matrix,window_filter_size)\n#sinchronized_matrix   = fps_sinchronize_frames(coordinated_matrix, video_path_timestamps, fps, max_frames)\n#video_coordinates            = synchronize_video_duration(coordinated_matrix,timestamp,fps,video_initial,duration)\n\n#print(coordinated_matrix)\n#print(coordinated_matrix.shape)\n#print(timestamp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:42:23.754526Z","iopub.execute_input":"2025-03-04T23:42:23.754891Z","iopub.status.idle":"2025-03-04T23:42:23.936472Z","shell.execute_reply.started":"2025-03-04T23:42:23.754859Z","shell.execute_reply":"2025-03-04T23:42:23.935785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#video_coordinates = synchronize_video_duration(coordinated_matrix,timestamp,fps,video_initial,duration)\n\n# Convertion from timestamp to fps\nfps_gps           = 1000 / timestamp\n\nvideo_coordinates = truncate_gps_data(coordinated_matrix, round(fps), round(fps_gps), video_initial, duration)\n\n#print(video_coordinates.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:42:26.278669Z","iopub.execute_input":"2025-03-04T23:42:26.279238Z","iopub.status.idle":"2025-03-04T23:42:26.308437Z","shell.execute_reply.started":"2025-03-04T23:42:26.279207Z","shell.execute_reply":"2025-03-04T23:42:26.307554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef find_highest_frame_number(folder_path):\n    highest_number = 0\n    pattern = re.compile(r\"frame_(\\d+)\\.png\")\n\n    for filename in os.listdir(folder_path):\n        match = pattern.match(filename)\n        if match:\n            number = int(match.group(1))\n            if number > highest_number:\n                highest_number = number\n\n    return highest_number\n\nchunk_reduction    = 4\nforward_color_path = '/kaggle/working/frames/'\nforward_depth_path = '/kaggle/working/output/frames/monocular/'\nforward_file_name  = 'frame_0'\nmax_frame          = find_highest_frame_number(forward_color_path)\n\npcd_chunks_accumul = map_generation_frames(forward_depth_path, forward_color_path, chunk_reduction, max_frame, fps, forward_file_name, coordinated_matrix)\n\nmap_chunks = []\nfor index in range(len(pcd_chunks_accumul)):\n    pcd_map_result = accumulate_pcd(pcd_chunks_accumul[index])\n    map_chunks.append(pcd_map_result)\n\npcd_map_result = accumulate_pcd(map_chunks)\n\nfor index in range(len(map_chunks)):\n    combined_pcd_path = 'camera_map_point_cloud' + str(index) + '.pcd'\n    o3d.io.write_point_cloud(combined_pcd_path, map_chunks[index])\n\nresult_map_result = 'point_cloud_completed.pcd'\no3d.io.write_point_cloud(result_map_result, pcd_map_result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:43:06.688652Z","iopub.execute_input":"2025-03-04T23:43:06.689328Z","iopub.status.idle":"2025-03-04T23:43:42.418331Z","shell.execute_reply.started":"2025-03-04T23:43:06.689298Z","shell.execute_reply":"2025-03-04T23:43:42.417469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport open3d as o3d\nimport sys\nimport math\nimport struct\nimport os\nimport copy\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport imageio.v3 as iio\nimport matplotlib.pyplot as plt\nimport open3d as o3d\nimport math\nimport copy\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport imageio.v3 as iio\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom PIL import Image\nfrom numpy import asarray\nfrom skimage.io import imread, imshow\nfrom skimage.color import rgb2hsv\n\ndef reduce_point_cloud_density(point_cloud_path, target_size_mb):\n    pcd = o3d.io.read_point_cloud(point_cloud_path)\n        \n    current_size_mb = os.path.getsize(point_cloud_path) / (1024 * 1024)\n\n    # Compute the number of points to keep\n    num_points           = len(pcd.points)\n    reduction_percentage = (1 - target_size_mb / current_size_mb) * 100\n    num_points_to_keep   = int(num_points * (1 - reduction_percentage / 100.0))\n\n    # Create an array of indices and shuffle it\n    indices = np.arange(num_points)\n    np.random.shuffle(indices)\n\n    # Select the subset of points\n    selected_indices = indices[:num_points_to_keep]\n\n    # Downsample the point cloud\n    downsampled_point_cloud = pcd.select_by_index(selected_indices)\n    \n    return downsampled_point_cloud\n\npcd_path = '/kaggle/working/point_cloud_completed.pcd'\ndownsampled_pcd = reduce_point_cloud_density(pcd_path, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:44:40.012934Z","iopub.execute_input":"2025-03-04T23:44:40.013553Z","iopub.status.idle":"2025-03-04T23:44:40.501151Z","shell.execute_reply.started":"2025-03-04T23:44:40.013519Z","shell.execute_reply":"2025-03-04T23:44:40.500281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def erosion(mask,kernel_size,iter_erosion):\n    kernel = np.ones((kernel_size,kernel_size),np.uint8)\n    erosion = cv2.erode(mask,kernel,iterations = iter_erosion)\n    return erosion\n\ndef opening(mask,kernel_size):\n    kernel = np.ones((kernel_size,kernel_size),np.uint8)\n    return cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n\ndef extract_signs_frames(image_path,image_orig, frame_count, accumulated_rect):\n    # .JPG are the segmented images. image_path\n    # .JPG are the original images image_orig\n    #image      = cv2.imread(image_path)\n    #image_p_or = cv2.imread(image_orig)\n    image      = Image.open(image_path)\n    image      = image.convert('RGB')\n    image      = np.array(image)\n    image      = image / 255.0\n    image_p_or = Image.open(image_orig)\n    image_p_or = image_p_or.convert('RGB')\n    image_p_or = np.array(image_p_or)\n    image_p_or = image_p_or / 255.0\n    #print(type(image))\n    hsv        = rgb2hsv(image)\n    \n    # Constraints to identify the color of traffic signs\n    #refer to hue channel (in the colorbar)\n    #lower_mask      = hsv[:,:,0] > 0.6\n    #refer to hue channel (in the colorbar)\n    #upper_mask      = hsv[:,:,0] < 0.8 \n    #refer to transparency channel (in the colorbar)\n    #saturation_mask = hsv[:,:,1] > 0.4\n    \n    # Iter 1\n    #refer to hue channel (in the colorbar)\n    #lower_mask      = hsv[:,:,0] > 0.7\n    #refer to hue channel (in the colorbar)\n    #upper_mask      = hsv[:,:,0] < 0.8 \n    #refer to transparency channel (in the colorbar)\n    #lower_saturation = hsv[:,:,1] > 0.65\n    #refer to transparency channel (in the colorbar)\n    #upper_saturation = hsv[:,:,1] < 0.75\n    \n    # Iter 2\n    #refer to hue channel (in the colorbar)\n    #lower_mask      = hsv[:,:,0] > 0.6 \n    #refer to hue channel (in the colorbar)\n    #upper_mask      = hsv[:,:,0] < 0.8\n    #refer to transparency channel (in the colorbar)\n    #lower_saturation = hsv[:,:,1] > 0.65\n    #refer to transparency channel (in the colorbar)\n    #upper_saturation = hsv[:,:,1] < 0.9\n    \n    # 06-28\n    #refer to hue channel (in the colorbar)\n    lower_mask      = hsv[:,:,0] > 0.6\n    #refer to hue channel (in the colorbar)\n    upper_mask      = hsv[:,:,0] < 0.8\n    #refer to transparency channel (in the colorbar)\n    lower_saturation = hsv[:,:,1] > 0.6\n    #refer to transparency channel (in the colorbar)\n    upper_saturation = hsv[:,:,1] < 0.7\n    \n    # This is to change the kernel size and eliminate outliers\n    #mask              = np.array(upper_mask*lower_mask*saturation_mask,dtype=np.uint8)\n    mask              = np.array(upper_mask * lower_mask * upper_saturation * lower_saturation,dtype=np.uint8)\n    kernel_size       = 6\n    iter_erosion      = 2\n    #mask              = erosion(mask,kernel_size,iter_erosion)\n    #mask              = opening(mask,kernel_size)\n    #plt.imshow(opening_mask)\n    #red               = image_p_or[:,:,0]*mask\n    #green             = image_p_or[:,:,1]*mask\n    #blue              = image_p_or[:,:,2]*mask\n    red = (image_p_or[:, :, 2] * mask * 255).astype(np.uint8)\n    green = (image_p_or[:, :, 1] * mask * 255).astype(np.uint8)\n    blue = (image_p_or[:, :, 0] * mask * 255).astype(np.uint8)\n    image_masked      = np.dstack((red,green,blue))\n    contours, _       = cv2.findContours(red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n    #To avoid outliers\n    #min_area          = 500\n    min_area          = 1000\n    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) >= min_area]\n    bounding_rects    = [cv2.boundingRect(cnt) for cnt in filtered_contours]\n    result_image      = image_masked.copy()\n    \n    output_dir        = 'cropped_images/'\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print('Frame has ' + str(len(bounding_rects)) + ' rectangles')\n    \n    log_file_path = output_dir + 'signs_location.txt'\n    # If it does not exist, create it\n    if not os.path.exists(log_file_path):\n        os.mknod(log_file_path)\n\n    # Save the individual cropped images\n    for i, rect in enumerate(bounding_rects):\n        x, y, w, h    = rect\n        cropped_image = image_masked[y:y+h, x:x+w]\n        output_path   = os.path.join(output_dir, f'cropped_{accumulated_rect + i}.jpg')\n        file          = open(log_file_path,'a')\n        file.write(str(frame_count) + '/' + str(accumulated_rect + i) + '\\n')\n        file.close()\n        cv2.imwrite(output_path, cropped_image)\n    \n    # It tells how many rectangles are there\n    return len(bounding_rects)\n    \ndef extract_signs_from_frames(forward_segm_path, forward_color_path, max_frame, forward_file_name_begin):\n    rectangles_accum = 0\n    \n    num_digits = len(str(max_frame))\n    \n    for index in range(max_frame):\n        print('Processing frame: ' +  str(index))\n        # For 1 and 4, 2 and 3 too after fix\n        forward_file_name_s = forward_file_name_begin + '_1' + str(index)\n        # For 2 and 3, this files have an error and when they were merge, fps change from 30 to 25.\n        # When fps are different\n        #forward_file_name_s = forward_file_name_begin + '_complete' + str(int(index * 25 / 30) + 1)\n        \n        formatted_number = f\"{(index):0{num_digits}d}\"\n        \n        #forward_file_name_c = forward_file_name_begin + str(index)\n        \n        forward_file_name_c = forward_file_name_begin + formatted_number\n        \n        color_file_name_f   = forward_color_path + forward_file_name_c + '.png'\n        segme_file_name_f   = forward_segm_path + forward_file_name_c + '.png'\n\n        # Get the segmented image with these names\n        rectangles_current  = extract_signs_frames(segme_file_name_f,color_file_name_f,index,rectangles_accum)\n        rectangles_accum    += rectangles_current\n            \ndef frame_extractor(input_video, output_dir, resize_width, resize_height):\n    video_local = cv2.VideoCapture(input_video)\n    os.makedirs(output_dir, exist_ok=True)\n    i = 0\n    while(video_local.isOpened()):\n        ret, frame = video_local.read()\n        if ret == False:\n            break\n        #frame = cv2.resize(frame, (resize_width, resize_height))\n        cv2.imwrite(output_dir + input_video.split(\"/\")[-1].split(\".\")[0] + str(i) + '.jpg',frame)\n        i+=1\n    video_local.release()\n    return i\n\ndef extract_signs(image_path, image_orig, output_path):\n    # Open the images and convert to RGB\n    image = Image.open(image_path).convert('RGB')\n    image = np.array(image) / 255.0  # Normalize to 0-1 range\n    image_p_or = Image.open(image_orig).convert('RGB')\n    image_p_or = np.array(image_p_or) / 255.0  # Normalize to 0-1 range\n\n    # Convert to HSV color space\n    hsv = rgb2hsv(image)\n\n    # Apply color filtering constraints\n    lower_mask = hsv[:, :, 0] > 0.6\n    upper_mask = hsv[:, :, 0] < 0.8\n    lower_saturation = hsv[:, :, 1] > 0.6\n    upper_saturation = hsv[:, :, 1] < 0.7\n\n    # Create the binary mask\n    mask = (upper_mask & lower_mask & upper_saturation & lower_saturation).astype(np.uint8)  # Convert to uint8\n\n    # Optional: Apply morphological operations if needed\n    kernel_size = 6\n    iter_erosion = 2\n    # mask = cv2.erode(mask, np.ones((kernel_size, kernel_size), np.uint8), iterations=iter_erosion)\n    # mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((kernel_size, kernel_size), np.uint8))\n\n    # Apply the mask to the original image\n    red = (image_p_or[:, :, 2] * mask * 255).astype(np.uint8)\n    green = (image_p_or[:, :, 1] * mask * 255).astype(np.uint8)\n    blue = (image_p_or[:, :, 0] * mask * 255).astype(np.uint8)\n    image_masked = np.dstack((red, green, blue))\n\n    # Save the masked image\n    cv2.imwrite(output_path, image_masked)\n\n    # Use only one channel (e.g., red channel) for contour detection\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\n    # Filter out small contours\n    min_area = 1000\n    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) >= min_area]\n\n    # Get bounding rectangles for the filtered contours\n    bounding_rects = [cv2.boundingRect(cnt) for cnt in filtered_contours]\n\n    return len(bounding_rects)\n\n\ndef increase_brightness(image_path, value):\n    # Read the image\n    image = cv2.imread(image_path)\n    # Check if image is loaded successfully\n    if image is None:\n        raise ValueError(f\"Image at path '{image_path}' could not be loaded.\")\n    # Convert the image to HSV (Hue, Saturation, Value) color space\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    # Split the image into three channels: H, S, and V\n    h, s, v = cv2.split(hsv)\n    # Increase the V (value/brightness) channel by the given value\n    v = cv2.add(v, value)\n    # Merge the channels back\n    final_hsv = cv2.merge((h, s, v))\n    # Convert the HSV image back to BGR color space\n    brightened_image = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n    return brightened_image\n\n#This is for Vision Transformers because it gives an inverse relation of white and black is.\ndef normalize_image_to_pcd(input_path,brightness_value):\n    brightness_image  = increase_brightness(input_path,brightness_value)\n    cv2.imwrite('brightened_image.jpg', brightness_image)\n    input_image       = Image.open('brightened_image.jpg').convert('L')\n    input_image_array = input_image.resize((640,480))\n    # It reverse depth making black, white and viceversa.\n    #max_pixel_value   = input_image_array\n    reverse_depth     = 255 - asarray(input_image_array)\n    #print(len(reverse_depth))\n    return(reverse_depth)\n\n#It extracts color from image.\ndef extract_color_image(img_path):\n    img = Image.open(img_path)\n    if img.mode != 'RGB':\n        img = img.convert('RGB')\n    if img.size != (640,480):\n        img = img.resize((640, 480))\n    img = asarray(img)\n    img = img/255.0 # normalize RGB values to [0, 1]\n    height, width, channels = img.shape\n    length = height * width\n\n    color_list = img.reshape((length, channels)) # array of RGB values\n    return color_list\n\n#It creates the PCD from depth.\ndef depth_to_pcd(depth_path, color_image_path, brightness_value, rectangles_found):\n    if (rectangles_found == 0):\n        pcd_o3d   = o3d.geometry.PointCloud()\n        exist_pcd = False\n    else:\n        # Depth camera parameters from NYU:\n        FX_DEPTH    = 5.8262448167737955e+02\n        FY_DEPTH    = 5.8269103270988637e+02\n        CX_DEPTH    = 3.1304475870804731e+02\n        CY_DEPTH    = 2.3844389626620386e+02\n        exist_pcd   = True\n        # It only has one channel it is in grey scale, it change size and invert greyscale\n        depth_image = normalize_image_to_pcd(depth_path,brightness_value)\n        # get depth resolution:\n        height, width = depth_image.shape\n        length = height * width\n        # compute indices:\n        jj = np.tile(range(width), height)\n        ii = np.repeat(range(height), width)\n        # rechape depth image\n        z = depth_image.reshape(length)\n        # compute pcd:\n        pcd = np.dstack([(ii - CX_DEPTH) * z / FX_DEPTH,\n                     (jj - CY_DEPTH) * z / FY_DEPTH,\n                     z]).reshape((length, 3))\n\n        pcd_o3d = o3d.geometry.PointCloud()\n        pcd_o3d.points = o3d.utility.Vector3dVector(pcd)\n        colors = extract_color_image(color_image_path)\n        pcd_o3d.colors = o3d.utility.Vector3dVector(colors)\n        #o3d.io.write_point_cloud(output_pcd_path, pcd_o3d)\n\n        # Define the box parameters\n        center   = np.array([0.0, 0.0, 250])\n        # Consider 70\n        extent   = np.array([400, 400, 90])\n        rotation = np.eye(3)  # Identity matrix for no rotation\n        # Cut the points inside the box, eliminate point in infinite or far far away\n        pcd_o3d  = cut_points_in_box(pcd_o3d, center, extent, rotation)\n    return pcd_o3d,exist_pcd\n\ndef cut_points_in_box(point_cloud, box_center, box_extent, box_rotation=None):\n    \"\"\"\n    Cut points inside a box from the point cloud.\n    \n    Parameters:\n    - point_cloud: open3d.geometry.PointCloud, the input point cloud.\n    - box_center: list or np.array of shape (3,), the center of the box.\n    - box_extent: list or np.array of shape (3,), the size of the box (x, y, z dimensions).\n    - box_rotation: optional, np.array of shape (3,3), the rotation matrix of the box.\n    \n    Returns:\n    - point_cloud_outside_box: open3d.geometry.PointCloud, the point cloud with points inside the box removed.\n    \"\"\"\n    \n    #points          = np.asarray(point_cloud.points)\n    #z_threshold     = 188\n    #mask            = points[:,2] < z_threshold    \n    #point_cloud.points = o3d.utility.Vector3dVector(points[mask]) # normals and colors are unchanged\n    \n    #return point_cloud\n    \n    # Create an AxisAlignedBoundingBox or OrientedBoundingBox\n    if box_rotation is None:\n        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=box_center - 0.5 * box_extent,\n                                                   max_bound=box_center + 0.5 * box_extent)\n    else:\n        bbox = o3d.geometry.OrientedBoundingBox(center=box_center,\n                                                R=box_rotation,\n                                                extent=box_extent)\n    \n    # Get the indices of points inside the box\n    indices_inside_box = bbox.get_point_indices_within_bounding_box(point_cloud.points)\n    \n    # Select points outside the box\n    point_cloud_outside_box = point_cloud.select_by_index(indices_inside_box, invert=True)\n    \n    return point_cloud_outside_box\n\ndef gps_to_decimeters(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Convert two GPS coordinates to meters relative to the first coordinate.\n    \n    :param lat1: Latitude of the first coordinate in decimal degrees\n    :param lon1: Longitude of the first coordinate in decimal degrees\n    :param lat2: Latitude of the second coordinate in decimal degrees\n    :param lon2: Longitude of the second coordinate in decimal degrees\n    :return: A tuple (x, y) representing the second coordinate in meters relative to the first\n    \"\"\"\n    # Radius of the Earth in meters\n    R = 6378137.0\n    \n    # Convert degrees to radians\n    lat1_rad = math.radians(lat1)\n    lon1_rad = math.radians(lon1)\n    lat2_rad = math.radians(lat2)\n    lon2_rad = math.radians(lon2)\n    \n    # Differences in coordinates\n    dlat = lat2_rad - lat1_rad\n    dlon = lon2_rad - lon1_rad\n    \n    # Equirectangular approximation\n    x_meters = dlon * math.cos((lat1_rad + lat2_rad) / 2.0) * R\n    y_meters = dlat * R\n    \n    # Convert to centimeters\n    x_decimeters = x_meters * 10\n    y_decimeters = y_meters * 10\n    \n    return x_decimeters, y_decimeters\n\ndef mean_filter_column(matrix, window_size):\n    \"\"\"\n    Apply a mean filter to each column of a matrix individually.\n    \n    Parameters:\n    matrix (np.ndarray): Input matrix of shape (n, m)\n    window_size (int): Size of the moving window for the mean filter\n    \n    Returns:\n    np.ndarray: Matrix with mean filter applied to each column\n    \"\"\"\n    if window_size < 1:\n        raise ValueError(\"window_size must be at least 1\")\n        \n    # Create a new matrix to store the results\n    filtered_matrix = np.zeros_like(matrix)\n    \n    # Get the number of rows and columns\n    n, m = matrix.shape\n    \n    # Apply the mean filter to each column\n    for col in range(m):\n        column = matrix[:, col]\n        filtered_column = np.zeros(n)\n        \n        # Apply the mean filter to the current column\n        for row in range(n):\n            start_idx = max(0, row - window_size // 2)\n            end_idx = min(n, row + window_size // 2 + 1)\n            filtered_column[row] = np.mean(column[start_idx:end_idx])\n        \n        filtered_matrix[:, col] = filtered_column\n    \n    return filtered_matrix\n\ndef vector_to_polar(x, y):\n    # Compute the magnitude\n    magnitude = math.sqrt(x**2 + y**2)\n    \n    # Compute the angle in radians\n    theta = math.atan2(y, x)  # atan2 returns the angle in radians\n\n    return magnitude, theta\n\ndef normalize_angle(angle):\n    \"\"\"\n    Normalize an angle to the range [-pi, pi].\n\n    Parameters:\n    angle (float): The angle in radians to normalize.\n\n    Returns:\n    float: The normalized angle in the range [-pi, pi].\n    \"\"\"\n    # Normalize the angle to the range [-pi, pi]\n    normalized_angle = (angle + math.pi) % (2 * math.pi) - math.pi\n    return normalized_angle\n\n# This function gives a coordinate system with relative movements and accumulated\ndef coordinated_system(gps_path):\n    # It eliminates first element to make the relative movement\n    file_gps_locations = open(gps_path, \"r\")\n    prior_element      = file_gps_locations.readline()\n    relative_magnitude = []\n    relative_theta     = []\n    # This keeps moving the origin and orientation\n    absolute_x_matrix  = []\n    absolute_x_scalar  = 0\n    absolute_y_matrix  = []\n    absolute_y_scalar  = 0\n    absolute_theta     = []\n    accumulated_theta  = 0\n    timestamp_matrix   = []\n    for line_gps in file_gps_locations:\n        timestamp_0        = int(prior_element.split('/')[0])\n        latitude_0         = float(prior_element.split('/')[1])\n        longitude_0        = float(prior_element.split('/')[2])\n        timestamp_1        = int(line_gps.split('/')[0])\n        latitude_1         = float(line_gps.split('/')[1])\n        longitude_1        = float(line_gps.split('/')[2])\n        # Update element\n        prior_element      = line_gps\n        x, y               = gps_to_decimeters(latitude_0, longitude_0, latitude_1, longitude_1)\n        magnitude, theta   = vector_to_polar(x, y)\n        # Accumulates relative trajectory.\n        relative_magnitude.append(magnitude)\n        # The sampling rate is approximately 30 Hz, then if the truck moves not even 1 cm in that period, then, it is still\n        if absolute_x_scalar < 0.1 and absolute_y_scalar < 0.1:\n            theta = 0\n        relative_theta.append(theta)\n        # It translates origin and accumulates theta\n        absolute_x_scalar  += x\n        absolute_y_scalar  += y\n        accumulated_theta  += theta\n        absolute_x_matrix.append(absolute_x_scalar)\n        absolute_y_matrix.append(absolute_y_scalar)\n        absolute_theta.append(accumulated_theta)\n        timestamp_matrix.append(timestamp_1)\n    # It returns matrix with accumulated values\n    result_matrix = relative_magnitude\n    result_matrix = np.vstack((result_matrix,relative_theta))\n    result_matrix = np.vstack((result_matrix,absolute_x_matrix))\n    result_matrix = np.vstack((result_matrix,absolute_y_matrix))\n    result_matrix = np.vstack((result_matrix,absolute_theta))\n    result_matrix = np.vstack((result_matrix,timestamp_matrix))\n    result_matrix = np.transpose(result_matrix)\n    return result_matrix\n\ndef fps_sinchronize(coordinate_matrix, video_path_timestamps, fps_video, video_duration_seconds):\n    file_video_l = open(video_path_timestamps,\"r\")\n    first_time_v = int(file_video_l.readline())\n    # It is used to calculate the required timestamps\n    frame_cal_ar = []\n    for frame_number in range(video_duration_seconds * fps_video):\n        frame_cal_ar.append(first_time_v + frame_number * 1e9 / fps_video)\n    magnitude_m  = []\n    rel_theta_m  = []\n    absolute_x_m = []\n    absolute_y_m = []\n    absolute_t_m = []\n    timestamp_m  = []\n    \n    log_file_path = 'log_timestamps_pcd.txt'\n    if not os.path.exists(log_file_path):\n        os.mknod(log_file_path)\n        \n    file = open(log_file_path,'a')\n    # Look for the frame with the lower difference\n    for frame_cal in frame_cal_ar:\n        # To initialize value\n        threshold      = frame_cal\n        magnitude      = 0\n        relative_theta = 0\n        absolute_x     = 0\n        absolute_y     = 0\n        absolute_theta = 0\n        timestamp      = 0\n        # Look through all the rows\n        for index in range(coordinate_matrix.shape[0]):\n            cal_time_stamp = abs(frame_cal - coordinate_matrix[index,5])\n            if(cal_time_stamp < threshold):\n                magnitude  = coordinate_matrix[index,0]\n                r_theta    = coordinate_matrix[index,1]\n                absolute_x = coordinate_matrix[index,2]\n                absolute_y = coordinate_matrix[index,3]\n                absolute_t = coordinate_matrix[index,4]\n                timestamp  = coordinate_matrix[index,5]\n                threshold  = cal_time_stamp\n        # Save the last value with shortest threshold.\n        magnitude_m.append(magnitude)\n        rel_theta_m.append(r_theta)\n        absolute_x_m.append(absolute_x)\n        absolute_y_m.append(absolute_y)\n        absolute_t_m.append(absolute_t)\n        timestamp_m.append(timestamp)\n        file.write(str(magnitude) + '/' + str(r_theta) + '/' + str(absolute_x) + '/' + str(absolute_y) + '/' + str(timestamp) + '\\n')\n        \n    result_matrix = magnitude_m\n    result_matrix = np.vstack((result_matrix,rel_theta_m))\n    result_matrix = np.vstack((result_matrix,absolute_x_m))\n    result_matrix = np.vstack((result_matrix,absolute_y_m))\n    result_matrix = np.vstack((result_matrix,absolute_t_m))\n    result_matrix = np.vstack((result_matrix,timestamp_m))\n    result_matrix = np.transpose(result_matrix)\n    file.close()\n    return result_matrix\n\ndef fps_sinchronize_frames(coordinate_matrix, video_path_timestamps, fps_video, video_duration_frames):\n    file_video_l = open(video_path_timestamps,\"r\")\n    file_vid_rl  = file_video_l.readline()\n    if(not(file_vid_rl.isnumeric())):\n        file_vid_rl = file_vid_rl.split('.')[0]\n    first_time_v = int(file_vid_rl)\n    # It is used to calculate the required timestamps\n    frame_cal_ar = []\n    for frame_number in range(video_duration_frames):\n        frame_cal_ar.append((first_time_v + frame_number / fps_video) * 1e9)\n    magnitude_m  = []\n    rel_theta_m  = []\n    absolute_x_m = []\n    absolute_y_m = []\n    absolute_t_m = []\n    timestamp_m  = []\n    \n    log_file_path = 'log_timestamps_pcd.txt'\n    if not os.path.exists(log_file_path):\n        os.mknod(log_file_path)\n        \n    file = open(log_file_path,'a')\n    # Look for the frame with the lower difference\n    for frame_cal in frame_cal_ar:\n        # To initialize value\n        threshold      = frame_cal\n        magnitude      = 0\n        r_theta        = 0\n        absolute_x     = 0\n        absolute_y     = 0\n        absolute_t     = 0\n        absolute_theta = 0\n        timestamp      = 0\n        # Look through all the rows\n        for index in range(coordinate_matrix.shape[0]):\n            cal_time_stamp = abs(frame_cal - coordinate_matrix[index,5])\n            if(cal_time_stamp < threshold):\n                magnitude  = coordinate_matrix[index,0]\n                r_theta    = coordinate_matrix[index,1]\n                absolute_x = coordinate_matrix[index,2]\n                absolute_y = coordinate_matrix[index,3]\n                absolute_t = coordinate_matrix[index,4]\n                timestamp  = coordinate_matrix[index,5]\n                threshold  = cal_time_stamp\n        # Save the last value with shortest threshold.\n        magnitude_m.append(magnitude)\n        rel_theta_m.append(r_theta)\n        absolute_x_m.append(absolute_x)\n        absolute_y_m.append(absolute_y)\n        absolute_t_m.append(absolute_t)\n        timestamp_m.append(timestamp)\n        file.write(str(magnitude) + '/' + str(r_theta) + '/' + str(absolute_x) + '/' + str(absolute_y) + '/' + str(timestamp) + '\\n')\n        \n    result_matrix = magnitude_m\n    result_matrix = np.vstack((result_matrix,rel_theta_m))\n    result_matrix = np.vstack((result_matrix,absolute_x_m))\n    result_matrix = np.vstack((result_matrix,absolute_y_m))\n    result_matrix = np.vstack((result_matrix,absolute_t_m))\n    result_matrix = np.vstack((result_matrix,timestamp_m))\n    result_matrix = np.transpose(result_matrix)\n    file.close()\n    return result_matrix\n\ndef load_point_clouds_rotated_translated(pcd_front_rear, theta, origin_x, origin_y, voxel_size=0.0):\n    #Rotations are relative to front or forward\n    rotate_rear            = (np.pi, 0, 0)\n    rotate_theta           = (theta, 0, 0)\n    # To change magnitude from centimeters to decimeters\n    translate_mt           = (0, origin_x, origin_y)\n    \n    # Calculate first front\n    mesh                   = copy.deepcopy(pcd_front_rear)\n    rotated_theta          = np.eye(4)\n    rotated_theta[:3, :3]  = pcd_front_rear.get_rotation_matrix_from_xyz(rotate_theta)\n    # Move pcd forward a magnitude and then rotated theta radians. In this order to remain forward\n    mesh                   = mesh.translate(translate_mt)\n    mesh                   = mesh.transform(rotated_theta)\n    pcd_down_front         = mesh.voxel_down_sample(voxel_size=voxel_size)\n    \n    # Calculate then rear\n    #rotated_matrix         = np.eye(4)\n    #rotated_matrix[:3, :3] = pcd_front_rear[1].get_rotation_matrix_from_xyz(rotate_rear)\n    #mesh                   = copy.deepcopy(pcd_front_rear[1]).transform(rotated_matrix)\n    #rotated_theta          = np.eye(4)\n    #rotated_theta[:3, :3]  = pcd_front_rear[1].get_rotation_matrix_from_xyz(rotate_theta)\n    # Rotate pcd forward and position it x and y decemiters. In this order to remain forward\n    #mesh                   = mesh.transform(rotated_theta)\n    #mesh                   = mesh.translate(translate_mt)\n    #pcd_down_rear          = mesh.voxel_down_sample(voxel_size=voxel_size)\n    \n    #return pcd_down_front + pcd_down_rear\n    # For now just collect front, rear has a pipe in the middle that will get noise into the scene\n    return pcd_down_front\n\ndef accumulate_pcd(pcds):\n    pcd_join_pcd = o3d.geometry.PointCloud()\n    for current_pcd in pcds:\n        pcd_join_pcd += current_pcd\n    # Returns PCD recollection \n    return pcd_join_pcd\n\ndef map_generation(forward_depth_path, forward_color_path, forward_segm_path, chunks_reduction, video_duration, fps, forward_file_name_begin, rear_file_name_begin, coordinated_matrix):\n    brightness_value   = 0\n    voxel_size         = 2\n    pcd_chunks_accumul = []\n\n    for part_of_map in range(chunks_reduction):\n        # To do it with a blank each time\n        pcd_accumulated = []\n        chunk_duration  = int(video_duration * fps / chunks_reduction)\n        offset          = part_of_map * chunk_duration\n        for index in range(chunk_duration):\n            print('Processing frame: ' +  str(offset + index))\n            pcd_studied             = []\n            forward_file_name       = forward_file_name_begin + str(offset + index)\n            rear_file_name          = rear_file_name_begin + str(offset + index)\n            depth_file_name_f       = forward_depth_path + forward_file_name + '.png'\n            color_file_name_f       = forward_color_path + forward_file_name + '.jpg'\n            segme_file_name_f       = forward_segm_path + forward_file_name + '.png'\n            depth_file_name_r       = forward_depth_path + forward_file_name + '.png'\n            color_file_name_r       = forward_color_path + forward_file_name + '.jpg'\n            segme_file_name_r       = forward_segm_path + forward_file_name + '.png'\n            # Get the segmented image with these names\n            segment_depth_out       = 'segmentation_depth.png'\n            rectangles_found        = extract_signs(segme_file_name_f,depth_file_name_f,segment_depth_out)\n            segment_color_out       = 'segmentation_color.png'\n            rectangles_found        = extract_signs(segme_file_name_f,color_file_name_f,segment_color_out)\n            # Accumulate front and then rear for the next function\n            theta                   = coordinated_matrix[offset + index,1]\n            origin_x                = coordinated_matrix[offset + index,2]\n            origin_y                = coordinated_matrix[offset + index,3]\n            generated_pcd,exist_pcd = depth_to_pcd(segment_depth_out, segment_color_out, brightness_value, rectangles_found)\n            if(exist_pcd):\n                pcd_studied.append(generated_pcd)\n                pcd_accumulated.append(load_point_clouds_rotated_translated(generated_pcd, theta, origin_x, origin_y, voxel_size))\n        pcd_chunks_accumul.append(pcd_accumulated)\n    return pcd_chunks_accumul\n\ndef map_generation_frames_segmentation(forward_depth_path, forward_color_path, forward_segm_path, chunks_reduction, video_duration_frames, fps, forward_file_name_begin, coordinated_matrix):\n    brightness_value   = 0\n    voxel_size         = 2\n    pcd_chunks_accumul = []\n    \n    num_digits = len(str(video_duration_frames))\n\n    for part_of_map in range(chunks_reduction):\n        # To do it with a blank each time\n        pcd_accumulated = []\n        chunk_duration  = int(video_duration_frames / chunks_reduction)\n        offset          = part_of_map * chunk_duration\n        for index in range(chunk_duration):\n            print('Processing frame: ' +  str(offset + index))\n            pcd_studied       = []\n            \n            # For 1 and 4\n            #forward_file_name_d = forward_file_name_begin + '_1' + str(offset + index)\n            # For 2 and 3\n            # When fps are different\n            #forward_file_name_d = forward_file_name_begin + '_complete' + str(int(index * 25 / 30) + 1)\n            #forward_file_name_d = forward_file_name_begin + '_complete' + str(index)\n            \n            formatted_number = f\"{(offset + index):0{num_digits}d}\"\n            \n            forward_file_name_d = forward_file_name_begin + formatted_number\n            # For 2 and 3\n            #forward_file_name_d = forward_file_name_begin + '_complete' + str(offset + index)\n            \n            forward_file_name_c = forward_file_name_d\n            depth_file_name_f   = forward_depth_path + forward_file_name_d + '.png'\n            color_file_name_f   = forward_color_path + forward_file_name_c + '.png'\n            segme_file_name_f   = forward_segm_path + forward_file_name_d + '.png'\n            \n            #After fixing fps from 2 and 3\n            #forward_file_name_d = forward_file_name_begin + '_1' + str(offset + index)\n            \n            #forward_file_name_c = forward_file_name_begin + str(offset + index)\n            \n            #depth_file_name_f   = forward_depth_path + forward_file_name_d + '.jpg'\n            #color_file_name_f   = forward_color_path + forward_file_name_c + '.jpg'\n            #segme_file_name_f   = forward_segm_path + forward_file_name_d + '.jpg'\n            \n            # Get the segmented image with these names\n            segment_depth_out       = 'segmentation_depth.png'\n            rectangles_found        = extract_signs(segme_file_name_f,depth_file_name_f,segment_depth_out)\n            segment_color_out       = 'segmentation_color.png'\n            rectangles_found        = extract_signs(segme_file_name_f,color_file_name_f,segment_color_out)\n            \n            # Accumulate front and then rear for the next function\n            theta                   = coordinated_matrix[offset + index,1]\n            origin_x                = coordinated_matrix[offset + index,2]\n            origin_y                = coordinated_matrix[offset + index,3]\n            generated_pcd,exist_pcd = depth_to_pcd(segment_depth_out, segment_color_out, brightness_value, rectangles_found)\n            if(exist_pcd):\n                pcd_studied.append(generated_pcd)\n                pcd_accumulated.append(load_point_clouds_rotated_translated(generated_pcd, theta, origin_x, origin_y, voxel_size))\n        pcd_chunks_accumul.append(pcd_accumulated)\n    return pcd_chunks_accumul\n\ndef frame_extractor(input_video, output_dir, resize_width, resize_height):\n    video_local = cv2.VideoCapture(input_video)\n    os.makedirs(output_dir, exist_ok=True)\n    i = 0\n    while(video_local.isOpened()):\n        ret, frame = video_local.read()\n        if ret == False:\n            break\n        #frame = cv2.resize(frame, (resize_width, resize_height))\n        cv2.imwrite(output_dir + input_video.split(\"/\")[-1].split(\".\")[0] + str(i) + '.jpg',frame)\n        i+=1\n    video_local.release()\n    return i","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:44:49.860765Z","iopub.execute_input":"2025-03-04T23:44:49.861340Z","iopub.status.idle":"2025-03-04T23:44:49.924704Z","shell.execute_reply.started":"2025-03-04T23:44:49.861306Z","shell.execute_reply":"2025-03-04T23:44:49.923907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forward_segm_path = '/kaggle/working/output/frames/segmentation/'\n\npcd_chunks_accumul = map_generation_frames_segmentation(forward_depth_path, forward_color_path, forward_segm_path, chunk_reduction, max_frame, fps, forward_file_name, coordinated_matrix)\n\nmap_chunks = []\nfor index in range(len(pcd_chunks_accumul)):\n    pcd_map_result = accumulate_pcd(pcd_chunks_accumul[index])\n    map_chunks.append(pcd_map_result)\n\npcd_map_result = accumulate_pcd(map_chunks)\n\nfor index in range(len(map_chunks)):\n    combined_pcd_path = 'camera_map_point_cloud' + str(index) + '.pcd'\n    o3d.io.write_point_cloud(combined_pcd_path, map_chunks[index])\n\nresult_map_result = 'point_cloud_completed_segmented.pcd'\no3d.io.write_point_cloud(result_map_result, pcd_map_result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:45:09.799104Z","iopub.execute_input":"2025-03-04T23:45:09.799905Z","iopub.status.idle":"2025-03-04T23:47:06.097985Z","shell.execute_reply.started":"2025-03-04T23:45:09.799869Z","shell.execute_reply":"2025-03-04T23:47:06.097081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def delete_folder_with_contents(folder_path):\n    \"\"\"\n    Deletes a specific folder and all of its contents.\n\n    Parameters:\n    folder_path (str): The full path to the folder you want to delete.\n    \"\"\"\n    if os.path.exists(folder_path):\n        try:\n            # Use shutil.rmtree to remove the folder and all of its contents\n            shutil.rmtree(folder_path)\n            print(f\"Successfully deleted folder: {folder_path}\")\n        except Exception as e:\n            print(f\"Error deleting folder {folder_path}: {e}\")\n    else:\n        print(f\"Folder not found: {folder_path}\")\n        \ndelete_folder_with_contents('/kaggle/working/DPT/')\n\nextract_signs_from_frames(forward_segm_path, forward_color_path, max_frame, 'frame_')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:21:47.581586Z","iopub.status.idle":"2025-03-04T23:21:47.581887Z","shell.execute_reply":"2025-03-04T23:21:47.581771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Writes coordinates\nfolder_path = '/kaggle/working/'\n\ndef write_matrix_shape_to_txt(matrix, filename):\n    # Write the matrix to a text file\n    np.savetxt(filename, matrix, fmt='%f')\n\ngps_coordinates = 'cropped_images/gps_coordinates_each_line_frame.txt'\nwrite_matrix_shape_to_txt(coordinated_matrix, gps_coordinates)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:21:47.582770Z","iopub.status.idle":"2025-03-04T23:21:47.583122Z","shell.execute_reply":"2025-03-04T23:21:47.582952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\ndef move_files_to_folder(files, destination_folder):\n    \"\"\"\n    Moves a list of files to a specified folder.\n\n    Parameters:\n    files (list of str): List of file paths to be moved.\n    destination_folder (str): Path to the destination folder.\n    \"\"\"\n    # Check if the destination folder exists, create it if it doesn't\n    if not os.path.exists(destination_folder):\n        os.makedirs(destination_folder)\n        print(f\"Created folder: {destination_folder}\")\n\n    for file in files:\n        if os.path.exists(file):\n            try:\n                shutil.move(file, destination_folder)\n                print(f\"Moved file: {file} to {destination_folder}\")\n            except Exception as e:\n                print(f\"Error moving file {file}: {e}\")\n        else:\n            print(f\"File not found: {file}\")\n            \ndef move_folders_to_folder(folders, destination_folder):\n    \"\"\"\n    Moves a list of folders and all their contents to a specified folder.\n\n    Parameters:\n    folders (list of str): List of folder paths to be moved.\n    destination_folder (str): Path to the destination folder.\n    \"\"\"\n    # Check if the destination folder exists, create it if it doesn't\n    if not os.path.exists(destination_folder):\n        os.makedirs(destination_folder)\n        print(f\"Created folder: {destination_folder}\")\n\n    for folder in folders:\n        if os.path.exists(folder):\n            try:\n                # Construct the new path in the destination folder\n                folder_name = os.path.basename(folder)\n                new_folder_path = os.path.join(destination_folder, folder_name)\n\n                # Move the folder\n                shutil.move(folder, new_folder_path)\n                print(f\"Moved folder: {folder} to {new_folder_path}\")\n            except Exception as e:\n                print(f\"Error moving folder {folder}: {e}\")\n        else:\n            print(f\"Folder not found: {folder}\")\n            \nfiles_to_move = [\"/kaggle/working/point_cloud_completed.pcd\", \"/kaggle/working/point_cloud_completed_segmented.pcd\"]\ndestination = \"/kaggle/working/work\"\nmove_files_to_folder(files_to_move, destination)\n\nfolders_to_move = [\"/kaggle/working/cropped_images/\"]\nmove_folders_to_folder(folders_to_move, destination)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:21:47.583845Z","iopub.status.idle":"2025-03-04T23:21:47.584282Z","shell.execute_reply":"2025-03-04T23:21:47.584099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folder_path = '/kaggle/working/'\n\ndef delete_all_except(folder_path, exception_folder):\n    \"\"\"\n    Deletes all contents in the specified folder except a specific subfolder and all files within that subfolder.\n\n    Parameters:\n    folder_path (str): The path to the main folder.\n    exception_folder (str): The name of the subfolder to keep (not the full path, just the folder name).\n    \"\"\"\n    if not os.path.exists(folder_path):\n        print(f\"Folder not found: {folder_path}\")\n        return\n\n    # Construct the full path to the exception folder\n    exception_folder_path = os.path.join(folder_path, exception_folder)\n    \n    # Check if the exception folder exists\n    if not os.path.exists(exception_folder_path) or not os.path.isdir(exception_folder_path):\n        print(f\"Exception folder not found or is not a directory: {exception_folder_path}\")\n        return\n\n    # List all items in the main folder\n    for item in os.listdir(folder_path):\n        item_path = os.path.join(folder_path, item)\n        \n        # Skip the exception folder\n        if item_path == exception_folder_path:\n            print(f\"Skipped folder: {item_path}\")\n            continue\n\n        # Delete the item (file or folder)\n        try:\n            if os.path.isfile(item_path):\n                os.remove(item_path)\n                print(f\"Deleted file: {item_path}\")\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n                print(f\"Deleted folder: {item_path}\")\n        except Exception as e:\n            print(f\"Error deleting {item_path}: {e}\")\n            \ndelete_all_except(folder_path, destination)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:22:10.810677Z","iopub.execute_input":"2025-03-04T23:22:10.811022Z","iopub.status.idle":"2025-03-04T23:22:10.824129Z","shell.execute_reply.started":"2025-03-04T23:22:10.810994Z","shell.execute_reply":"2025-03-04T23:22:10.822638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')\n\n!tar -czf combined.tar.gz *\n\nfrom IPython.display import FileLink\n\nFileLink(r'combined.tar.gz')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:21:47.586269Z","iopub.status.idle":"2025-03-04T23:21:47.586681Z","shell.execute_reply":"2025-03-04T23:21:47.586501Z"}},"outputs":[],"execution_count":null}]}